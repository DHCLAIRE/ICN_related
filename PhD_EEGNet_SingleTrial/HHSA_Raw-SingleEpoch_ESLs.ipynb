{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7237917c-db70-442b-97d1-8d04eaee9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3d1c9c24-9a0f-450f-8360-dcb4186d8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import eelbrain\n",
    "import mne\n",
    "#import trftools\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import emd  # Ensure you ran: pip install EMD-signal\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "68a6da85-e566-4830-8c9d-40c93a8ef389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_2_S030_ICAed_raw.fif', 'n_2_S027_ICAed_raw.fif', 'n_2_S023_ICAed_raw.fif', 'n_2_S034_ICAed_raw.fif', 'n_2_S024_ICAed_raw.fif', 'n_2_S019_ICAed_raw.fif', 'n_2_S020_ICAed_raw.fif', 'n_2_S013_ICAed_raw.fif', 'n_2_S017_ICAed_raw.fif', 'n_2_S039_ICAed_raw.fif', 'n_2_S010_ICAed_raw.fif', 'n_2_S029_ICAed_raw.fif', 'n_2_S015_ICAed_raw.fif', 'n_2_S028_ICAed_raw.fif', 'n_2_S011_ICAed_raw.fif', 'n_2_S038_ICAed_raw.fif', 'n_2_S016_ICAed_raw.fif', 'n_2_S012_ICAed_raw.fif', 'n_2_S021_ICAed_raw.fif', 'n_2_S036_ICAed_raw.fif', 'n_2_S032_ICAed_raw.fif', 'n_2_S025_ICAed_raw.fif', 'n_2_S035_ICAed_raw.fif', 'n_2_S022_ICAed_raw.fif', 'n_2_S026_ICAed_raw.fif', 'n_2_S031_ICAed_raw.fif']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "## ESLs ##\n",
    "## Import the raw EEG data of ESLs(Alice)\n",
    "\n",
    "STIMULI = [str(i) for i in range(1, 13)]\n",
    "DATA_ROOT = Path(\"/Users/neuroling/Downloads/DINGHSIN_Results/Alice_Experiments_Results\")\n",
    "#DATA_ROOT = Path(\"/Volumes/Neurolang_1/Master Program/New_Thesis_topic/Experiments_Results\")  #Path(\"~\").expanduser() / 'Data' / 'Alice'\n",
    "PREDICTOR_audio_DIR = DATA_ROOT / 'TRFs_pridictors/audio_predictors'\n",
    "PREDICTOR_word_DIR = DATA_ROOT / 'TRFs_pridictors/word_predictors'\n",
    "EEG_DIR = DATA_ROOT / 'EEG_ESLs' / 'Alice_ESL_ICAed_fif'\n",
    "ESL_SUBJECTS = [path.name for path in EEG_DIR.iterdir() if re.match(r'n_2_S\\d*', path.name)]  #S01_alice-raw.fif\n",
    "# Define a target directory for TRF estimates and make sure the directory is created\n",
    "TRF_DIR = DATA_ROOT / 'TRFs_ESLs'\n",
    "TRF_DIR.mkdir(exist_ok=True)\n",
    "print(ESL_SUBJECTS)\n",
    "print(len(ESL_SUBJECTS))  # 26\n",
    "\n",
    "DST = TRF_DIR / 'ESLs_figures'\n",
    "DST.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4745a1e4-b314-4eef-8bcd-33744acd0daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S44_Alice-natives_sfreq-100_raw.fif', 'S20_Alice-natives_sfreq-100_raw.fif', 'S13_Alice-natives_sfreq-100_raw.fif', 'S01_Alice-natives_sfreq-100_raw.fif', 'S16_Alice-natives_sfreq-100_raw.fif', 'S41_Alice-natives_sfreq-100_raw.fif', 'S25_Alice-natives_sfreq-100_raw.fif', 'S37_Alice-natives_sfreq-100_raw.fif', 'S04_Alice-natives_sfreq-100_raw.fif', 'S18_Alice-natives_sfreq-100_raw.fif', 'S39_Alice-natives_sfreq-100_raw.fif', 'S10_Alice-natives_sfreq-100_raw.fif', 'S15_Alice-natives_sfreq-100_raw.fif', 'S26_Alice-natives_sfreq-100_raw.fif', 'S42_Alice-natives_sfreq-100_raw.fif', 'S34_Alice-natives_sfreq-100_raw.fif', 'S38_Alice-natives_sfreq-100_raw.fif', 'S19_Alice-natives_sfreq-100_raw.fif', 'S06_Alice-natives_sfreq-100_raw.fif', 'S35_Alice-natives_sfreq-100_raw.fif', 'S14_Alice-natives_sfreq-100_raw.fif', 'S03_Alice-natives_sfreq-100_raw.fif', 'S11_Alice-natives_sfreq-100_raw.fif', 'S22_Alice-natives_sfreq-100_raw.fif', 'S05_Alice-natives_sfreq-100_raw.fif', 'S36_Alice-natives_sfreq-100_raw.fif', 'S40_Alice-natives_sfreq-100_raw.fif', 'S17_Alice-natives_sfreq-100_raw.fif', 'S12_Alice-natives_sfreq-100_raw.fif', 'S45_Alice-natives_sfreq-100_raw.fif', 'S21_Alice-natives_sfreq-100_raw.fif', 'S48_Alice-natives_sfreq-100_raw.fif', 'S08_Alice-natives_sfreq-100_raw.fif']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "## Natives ##\n",
    "## Import the raw EEG data of ESLs(Alice)\n",
    "\n",
    "STIMULI = [str(i) for i in range(1, 13)]\n",
    "DATA_ROOT = Path(\"/Users/neuroling/Downloads/DINGHSIN_Results/Alice_Experiments_Results\") #Path(\"/Volumes/Neurolang_1/Master Program/New_Thesis_topic/Experiments_Results\")  #Path(\"~\").expanduser() / 'Data' / 'Alice'\n",
    "PREDICTOR_audio_DIR = DATA_ROOT / 'TRFs_pridictors/audio_predictors'\n",
    "PREDICTOR_word_DIR = DATA_ROOT / 'TRFs_pridictors/word_predictors'\n",
    "IMF_DIR = DATA_ROOT/ \"TRFs_pridictors/IF_predictors\"\n",
    "F0_DIR = DATA_ROOT/ \"TRFs_pridictors/F0_predictors\"\n",
    "IMFsLIST = [path.name for path in IMF_DIR.iterdir() if re.match(r'Alice_IF_IMF_*', path.name)]\n",
    "EEG_DIR = DATA_ROOT / 'EEG_Natives' / 'Alice_natives_ICAed_fif'\n",
    "SUBJECTS = [path.name for path in EEG_DIR.iterdir() if re.match(r'S\\d*', path.name[:4])]\n",
    "# Define a target directory for TRF estimates and make sure the directory is created\n",
    "TRF_DIR = DATA_ROOT / 'TRFs_Natives'\n",
    "TRF_DIR.mkdir(exist_ok=True)\n",
    "print(SUBJECTS)\n",
    "print(len(SUBJECTS))\n",
    "\n",
    "wOnset_DIR = DATA_ROOT / 'EEG_Natives' / 'Alice_Natives_wOnset_raw_epochs'\n",
    "wOnset_DIR .mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "45be25b7-0a68-4d75-a6f6-1b70a57e1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "2\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "3\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "4\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "5\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "6\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "7\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "8\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "9\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "10\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "11\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "12\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import the csv data\n",
    "csv_data = DATA_ROOT / \"Alice(EEG_mat_and stimuli)\" / \"AliceChapterOne-EEG.csv\"  # self-made LMM data form\n",
    "\n",
    "\n",
    "word_onset_LIST = []\n",
    "with open(csv_data, \"r\", encoding=\"UTF-8\") as f:\n",
    "    fileDF = pd.read_csv(f, sep=\",\")\n",
    "    #print(fileDF.columns)\n",
    "    # word_onset = fileDF[\"onset\"]\n",
    "    # print(word_onset, type(word_onset))\n",
    "\n",
    "    word_onset_essentials_DF = fileDF.iloc[:,[0, 1, 2] ]     # first column\n",
    "    #print(word_onset_essentials_DF[\"onset\"], word_onset_essentials_DF[\"onset\"])\n",
    "    #print()\n",
    "\n",
    "    for i in range(1, 13):\n",
    "        print(i)\n",
    "        #w_S = word_onset_essentials_DF.loc[:, [i][\"Word\"]]\n",
    "        #wOnset_F = word_onset_essentials_DF.loc[:, 2]\n",
    "        #print(w_S)\n",
    "\n",
    "        wOnset_DF = word_onset_essentials_DF.loc[word_onset_essentials_DF[\"Segment\"] == 2, :]\n",
    "        print(wOnset_DF, type(wOnset_DF))\n",
    "        print(wOnset_DF[\"onset\"])\n",
    "        \"\"\"\n",
    "        if \n",
    "        w_S = word_onset_essentials_DF.iloc[i, 0]\n",
    "        wOnset_F = word_onset_essentials_DF.iloc[i, 2]\n",
    "        print(w_S, wOnset_F)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "10ab0499-ecba-460c-97c7-93074b333aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n",
      "Factor(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], name='event')\n"
     ]
    }
   ],
   "source": [
    "for subject in SUBJECTS:#[0:3]:\n",
    "    #print(\"subject_num=\", subject)\n",
    "    \n",
    "    # 1. Load Raw as an Eelbrain-compatible object\n",
    "    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)\n",
    "    raw_sfreq = raw.info['sfreq']\n",
    "    \n",
    "    # 2. Get the events for the 12 tapes\n",
    "    events_DICT = eelbrain.load.mne.events(raw)\n",
    "    print(events_DICT[\"event\"])\n",
    "\n",
    "    \"\"\"\n",
    "    if len(events_DICT[\"event\"])==12:\n",
    "        pass\n",
    "    else:\n",
    "        print(subject[:3], \"tape_length\", )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bc4553ed-852e-47c9-be5b-c28dd2079f03",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (2029598978.py, line 113)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[194], line 113\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "## (Use this!!!)Trying the old way (Sub order 1st then tape 2nd)##\n",
    "\n",
    "# epochs parameters\n",
    "# Define your epoch window\n",
    "# We use a 1.5s window (including buffers for HHSA)\n",
    "tmin, tmax = -0.3, 1.2 \n",
    "\n",
    "for subject in SUBJECTS:#[0:3]:\n",
    "    print(\"subject_num=\", subject)\n",
    "    \n",
    "    # 1. Load Raw as an Eelbrain-compatible object\n",
    "    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)\n",
    "    raw_sfreq = raw.info['sfreq']\n",
    "    \n",
    "    # 2. Get the events for the 12 tapes\n",
    "    events_DICT = eelbrain.load.mne.events(raw)\n",
    "\n",
    "    try:\n",
    "        if len(events_DICT['event'])==12:\n",
    "            # Get the actual word onset based on EEG triggers datapoints\n",
    "            all_tapes_epochs_LIST = []\n",
    "            for i, stimulus_idx in enumerate(trial_indexes):\n",
    "                print(\"tape_num=\", i+1)\n",
    "                \n",
    "                # Find the word onset time based on the segment sequence    \n",
    "                wOnset_perTape_DF = word_onset_essentials_DF.loc[word_onset_essentials_DF[\"Segment\"] == stimulus_idx+1, :] #.to_numpy()\n",
    "                print(\"wOnset_perTape_DF=\", wOnset_perTape_DF)\n",
    "                #wOnset_time_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy() #*raw_sfreq\n",
    "                wOnset_datapoints_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy()*raw_sfreq\n",
    "                print(\"wOnset_datapoints_ndarray=\", wOnset_datapoints_ndarray[0:15], type(wOnset_datapoints_ndarray))\n",
    "        \n",
    "                # Get the tape start time\n",
    "                tape_start_datapoints_npINT64 = events_DICT[stimulus_idx]['i_start']\n",
    "                #tape_start_time_npFLOAT64 = tape_start_datapoints_npINT64 / raw_sfreq\n",
    "                print(\"tape_start_datapoints_npINT64=\", tape_start_datapoints_npINT64)\n",
    "                #print(\"tape_start_time_npFLOAT64=\", tape_start_time_npFLOAT64, type(tape_start_time_npFLOAT64))\n",
    "        \n",
    "                # Get the actual word onset time by the triggers\n",
    "                #absolute_onsets_time_ndarray = tape_start_time_npFLOAT64 + wOnset_time_ndarray\n",
    "                #print(\"absolute_onsets_time_ndarray=\", absolute_onsets_time_ndarray[0:15])\n",
    "                abs_wOnsets_dta_ndarray = tape_start_datapoints_npINT64 + wOnset_datapoints_ndarray\n",
    "                print(\"abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "                \n",
    "                # To exclude the decimal but leave the integer along, and turn FLOAT into INT\n",
    "                abs_wOnsets_dta_ndarray = np.trunc(abs_wOnsets_dta_ndarray).astype(int)\n",
    "                print(\"rounded_abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "                \n",
    "                # Make epochs\n",
    "                # Create the empty (N, 3) event matrix based on wOnset per tape\n",
    "                wOnset_events = len(abs_wOnsets_dta_ndarray)\n",
    "                wOnset_perTape_events = np.zeros((wOnset_events, 3), dtype=int)\n",
    "                # Fill the columns\n",
    "                wOnset_perTape_events[:, 0] = abs_wOnsets_dta_ndarray  # Column 0: The sample indices\n",
    "                wOnset_perTape_events[:, 2] = i+1           # Column 2: The event ID (e.g., 1)\n",
    "                print(wOnset_perTape_events)\n",
    "                \n",
    "                #word_perTape_epochs = mne.epochs(raw, tmin=tmin, tmax=tmax, baseline=None, events=wOnset_perTape_events)\n",
    "                tape_perTape_epochs = mne.Epochs(raw, events=wOnset_perTape_events, event_id=i+1, tmin=tmin, tmax=tmax, baseline=None, preload=True)\n",
    "                print(tape_perTape_epochs)\n",
    "                all_tapes_epochs_LIST.append(tape_perTape_epochs)\n",
    "                wOnset_epochs = mne.concatenate_epochs(all_tapes_epochs_LIST)\n",
    "            wOnset_epochs.save(wOnset_DIR / Path('%s_Natives_wOnset_epochs_allTapes_raw.fif' %subject[:3]), overwrite=True)\n",
    "            print(\"subject_num=\", subject[:4], \"wOnset epoch saved.\")\n",
    "        \n",
    "        else:  # For those who only start on the 2nd tape\n",
    "            # Get the actual word onset based on EEG triggers datapoints\n",
    "            all_tapes_epochs_LIST = []\n",
    "            for i, stimulus_idx in enumerate(trial_indexes):\n",
    "                print(\"tape_num=\", i+2)\n",
    "                \n",
    "                # Find the word onset time based on the segment sequence    \n",
    "                wOnset_perTape_DF = word_onset_essentials_DF.loc[word_onset_essentials_DF[\"Segment\"] == stimulus_idx+2, :] #.to_numpy()\n",
    "                print(\"wOnset_perTape_DF=\", wOnset_perTape_DF)\n",
    "                #wOnset_time_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy() #*raw_sfreq\n",
    "                wOnset_datapoints_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy()*raw_sfreq\n",
    "                print(\"wOnset_datapoints_ndarray=\", wOnset_datapoints_ndarray[0:15], type(wOnset_datapoints_ndarray))\n",
    "        \n",
    "                # Get the tape start time\n",
    "                tape_start_datapoints_npINT64 = events_DICT[stimulus_idx]['i_start']\n",
    "                #tape_start_time_npFLOAT64 = tape_start_datapoints_npINT64 / raw_sfreq\n",
    "                print(\"tape_start_datapoints_npINT64=\", tape_start_datapoints_npINT64)\n",
    "                #print(\"tape_start_time_npFLOAT64=\", tape_start_time_npFLOAT64, type(tape_start_time_npFLOAT64))\n",
    "        \n",
    "                # Get the actual word onset time by the triggers\n",
    "                #absolute_onsets_time_ndarray = tape_start_time_npFLOAT64 + wOnset_time_ndarray\n",
    "                #print(\"absolute_onsets_time_ndarray=\", absolute_onsets_time_ndarray[0:15])\n",
    "                abs_wOnsets_dta_ndarray = tape_start_datapoints_npINT64 + wOnset_datapoints_ndarray\n",
    "                print(\"abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "                \n",
    "                # To exclude the decimal but leave the integer along, and turn FLOAT into INT\n",
    "                abs_wOnsets_dta_ndarray = np.trunc(abs_wOnsets_dta_ndarray).astype(int)\n",
    "                print(\"rounded_abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "                \n",
    "                # Make epochs\n",
    "                # Create the empty (N, 3) event matrix based on wOnset per tape\n",
    "                wOnset_events = len(abs_wOnsets_dta_ndarray)\n",
    "                wOnset_perTape_events = np.zeros((wOnset_events, 3), dtype=int)\n",
    "                # Fill the columns\n",
    "                wOnset_perTape_events[:, 0] = abs_wOnsets_dta_ndarray  # Column 0: The sample indices\n",
    "                wOnset_perTape_events[:, 2] = i+2           # Column 2: The event ID (e.g., 1)\n",
    "                print(wOnset_perTape_events)\n",
    "                \n",
    "                #word_perTape_epochs = mne.epochs(raw, tmin=tmin, tmax=tmax, baseline=None, events=wOnset_perTape_events)\n",
    "                tape_perTape_epochs = mne.Epochs(raw, events=wOnset_perTape_events, event_id=i+1, tmin=tmin, tmax=tmax, baseline=None, preload=True)\n",
    "                print(tape_perTape_epochs)\n",
    "                all_tapes_epochs_LIST.append(tape_perTape_epochs)\n",
    "                wOnset_epochs = mne.concatenate_epochs(all_tapes_epochs_LIST)\n",
    "            wOnset_epochs.save(wOnset_DIR / Path('%s_Natives_wOnset_epochs_11Tapes_raw.fif' %subject[:3]), overwrite=True)\n",
    "            print(\"subject_num=\", subject[:4], \"wOnset epoch saved.\")\n",
    "        \n",
    "           \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "    # 5. Combine all words from all tapes into one Dataset\n",
    "    # This 'ds' will have a column for 'EEG' and can have a column for 'Subject'\n",
    "    ds_all_words = eelbrain.combine(all_tapes_epochs)\n",
    "    \n",
    "    # Now you can access the data for HHSA:\n",
    "    # eeg_data = ds_all_words['eeg'].get_data()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ef6d81d-35f7-4656-8353-4a451591a71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wOnset_datapoints_ndarray= [  4.6     56.2721  78.4543 125.6929 135.2925 161.6327 231.0749 279.9918\n",
      " 293.8712 330.449  348.4082 365.4596 410.6667 427.4286 439.8799] <class 'numpy.ndarray'>\n",
      "subject_num= S44_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "tape_start_datapoints_npINT64= 172\n",
      "abs_wOnsets_dta_ndarray= [176.6    228.2721 250.4543 297.6929 307.2925 333.6327 403.0749 451.9918\n",
      " 465.8712 502.449  520.4082 537.4596 582.6667 599.4286 611.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [176 228 250 297 307 333 403 451 465 502 520 537 582 599 611]\n",
      "[[ 176    0    1]\n",
      " [ 228    0    1]\n",
      " [ 250    0    1]\n",
      " [ 297    0    1]\n",
      " [ 307    0    1]\n",
      " [ 333    0    1]\n",
      " [ 403    0    1]\n",
      " [ 451    0    1]\n",
      " [ 465    0    1]\n",
      " [ 502    0    1]\n",
      " [ 520    0    1]\n",
      " [ 537    0    1]\n",
      " [ 582    0    1]\n",
      " [ 599    0    1]\n",
      " [ 611    0    1]\n",
      " [ 650    0    1]\n",
      " [ 662    0    1]\n",
      " [ 675    0    1]\n",
      " [ 715    0    1]\n",
      " [ 761    0    1]\n",
      " [ 777    0    1]\n",
      " [ 886    0    1]\n",
      " [ 910    0    1]\n",
      " [ 921    0    1]\n",
      " [ 967    0    1]\n",
      " [ 998    0    1]\n",
      " [1028    0    1]\n",
      " [1049    0    1]\n",
      " [1060    0    1]\n",
      " [1095    0    1]\n",
      " [1103    0    1]\n",
      " [1142    0    1]\n",
      " [1168    0    1]\n",
      " [1253    0    1]\n",
      " [1264    0    1]\n",
      " [1273    0    1]\n",
      " [1301    0    1]\n",
      " [1314    0    1]\n",
      " [1364    0    1]\n",
      " [1375    0    1]\n",
      " [1460    0    1]\n",
      " [1473    0    1]\n",
      " [1549    0    1]\n",
      " [1566    0    1]\n",
      " [1583    0    1]\n",
      " [1594    0    1]\n",
      " [1612    0    1]\n",
      " [1644    0    1]\n",
      " [1654    0    1]\n",
      " [1675    0    1]\n",
      " [1715    0    1]\n",
      " [1756    0    1]\n",
      " [1795    0    1]\n",
      " [1828    0    1]\n",
      " [1882    0    1]\n",
      " [1898    0    1]\n",
      " [2124    0    1]\n",
      " [2144    0    1]\n",
      " [2164    0    1]\n",
      " [2189    0    1]\n",
      " [2264    0    1]\n",
      " [2274    0    1]\n",
      " [2298    0    1]\n",
      " [2322    0    1]\n",
      " [2388    0    1]\n",
      " [2413    0    1]\n",
      " [2436    0    1]\n",
      " [2445    0    1]\n",
      " [2464    0    1]\n",
      " [2510    0    1]\n",
      " [2523    0    1]\n",
      " [2531    0    1]\n",
      " [2561    0    1]\n",
      " [2588    0    1]\n",
      " [2608    0    1]\n",
      " [2620    0    1]\n",
      " [2645    0    1]\n",
      " [2677    0    1]\n",
      " [2726    0    1]\n",
      " [2740    0    1]\n",
      " [2859    0    1]\n",
      " [2886    0    1]\n",
      " [2903    0    1]\n",
      " [2949    0    1]\n",
      " [2960    0    1]\n",
      " [2997    0    1]\n",
      " [3009    0    1]\n",
      " [3053    0    1]\n",
      " [3101    0    1]\n",
      " [3119    0    1]\n",
      " [3132    0    1]\n",
      " [3159    0    1]\n",
      " [3167    0    1]\n",
      " [3208    0    1]\n",
      " [3217    0    1]\n",
      " [3256    0    1]\n",
      " [3274    0    1]\n",
      " [3289    0    1]\n",
      " [3323    0    1]\n",
      " [3330    0    1]\n",
      " [3455    0    1]\n",
      " [3475    0    1]\n",
      " [3579    0    1]\n",
      " [3591    0    1]\n",
      " [3628    0    1]\n",
      " [3669    0    1]\n",
      " [3686    0    1]\n",
      " [3723    0    1]\n",
      " [3766    0    1]\n",
      " [3787    0    1]\n",
      " [3828    0    1]\n",
      " [3848    0    1]\n",
      " [4018    0    1]\n",
      " [4035    0    1]\n",
      " [4052    0    1]\n",
      " [4089    0    1]\n",
      " [4118    0    1]\n",
      " [4176    0    1]\n",
      " [4249    0    1]\n",
      " [4265    0    1]\n",
      " [4354    0    1]\n",
      " [4376    0    1]\n",
      " [4392    0    1]\n",
      " [4433    0    1]\n",
      " [4452    0    1]\n",
      " [4462    0    1]\n",
      " [4485    0    1]\n",
      " [4519    0    1]\n",
      " [4550    0    1]\n",
      " [4567    0    1]\n",
      " [4577    0    1]\n",
      " [4587    0    1]\n",
      " [4618    0    1]\n",
      " [4635    0    1]\n",
      " [4652    0    1]\n",
      " [4662    0    1]\n",
      " [4697    0    1]\n",
      " [4721    0    1]\n",
      " [4740    0    1]\n",
      " [4852    0    1]\n",
      " [4865    0    1]\n",
      " [4937    0    1]\n",
      " [4949    0    1]\n",
      " [5002    0    1]\n",
      " [5009    0    1]\n",
      " [5029    0    1]\n",
      " [5043    0    1]\n",
      " [5178    0    1]\n",
      " [5190    0    1]\n",
      " [5208    0    1]\n",
      " [5232    0    1]\n",
      " [5244    0    1]\n",
      " [5277    0    1]\n",
      " [5344    0    1]\n",
      " [5353    0    1]\n",
      " [5393    0    1]\n",
      " [5405    0    1]\n",
      " [5427    0    1]\n",
      " [5464    0    1]\n",
      " [5487    0    1]\n",
      " [5510    0    1]\n",
      " [5527    0    1]\n",
      " [5551    0    1]\n",
      " [5584    0    1]\n",
      " [5608    0    1]\n",
      " [5641    0    1]\n",
      " [5654    0    1]\n",
      " [5677    0    1]\n",
      " [5686    0    1]\n",
      " [5734    0    1]\n",
      " [5745    0    1]\n",
      " [5760    0    1]\n",
      " [5791    0    1]\n",
      " [5815    0    1]]\n",
      "<Epochs |  174 events (all good), -0.3 – 1.2 s, baseline off, ~11.9 MB, data loaded,\n",
      " '1': 174>\n",
      "subject_num= S20_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "tape_start_datapoints_npINT64= 413\n",
      "abs_wOnsets_dta_ndarray= [417.6    469.2721 491.4543 538.6929 548.2925 574.6327 644.0749 692.9918\n",
      " 706.8712 743.449  761.4082 778.4596 823.6667 840.4286 852.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [417 469 491 538 548 574 644 692 706 743 761 778 823 840 852]\n",
      "[[ 417    0    1]\n",
      " [ 469    0    1]\n",
      " [ 491    0    1]\n",
      " [ 538    0    1]\n",
      " [ 548    0    1]\n",
      " [ 574    0    1]\n",
      " [ 644    0    1]\n",
      " [ 692    0    1]\n",
      " [ 706    0    1]\n",
      " [ 743    0    1]\n",
      " [ 761    0    1]\n",
      " [ 778    0    1]\n",
      " [ 823    0    1]\n",
      " [ 840    0    1]\n",
      " [ 852    0    1]\n",
      " [ 891    0    1]\n",
      " [ 903    0    1]\n",
      " [ 916    0    1]\n",
      " [ 956    0    1]\n",
      " [1002    0    1]\n",
      " [1018    0    1]\n",
      " [1127    0    1]\n",
      " [1151    0    1]\n",
      " [1162    0    1]\n",
      " [1208    0    1]\n",
      " [1239    0    1]\n",
      " [1269    0    1]\n",
      " [1290    0    1]\n",
      " [1301    0    1]\n",
      " [1336    0    1]\n",
      " [1344    0    1]\n",
      " [1383    0    1]\n",
      " [1409    0    1]\n",
      " [1494    0    1]\n",
      " [1505    0    1]\n",
      " [1514    0    1]\n",
      " [1542    0    1]\n",
      " [1555    0    1]\n",
      " [1605    0    1]\n",
      " [1616    0    1]\n",
      " [1701    0    1]\n",
      " [1714    0    1]\n",
      " [1790    0    1]\n",
      " [1807    0    1]\n",
      " [1824    0    1]\n",
      " [1835    0    1]\n",
      " [1853    0    1]\n",
      " [1885    0    1]\n",
      " [1895    0    1]\n",
      " [1916    0    1]\n",
      " [1956    0    1]\n",
      " [1997    0    1]\n",
      " [2036    0    1]\n",
      " [2069    0    1]\n",
      " [2123    0    1]\n",
      " [2139    0    1]\n",
      " [2365    0    1]\n",
      " [2385    0    1]\n",
      " [2405    0    1]\n",
      " [2430    0    1]\n",
      " [2505    0    1]\n",
      " [2515    0    1]\n",
      " [2539    0    1]\n",
      " [2563    0    1]\n",
      " [2629    0    1]\n",
      " [2654    0    1]\n",
      " [2677    0    1]\n",
      " [2686    0    1]\n",
      " [2705    0    1]\n",
      " [2751    0    1]\n",
      " [2764    0    1]\n",
      " [2772    0    1]\n",
      " [2802    0    1]\n",
      " [2829    0    1]\n",
      " [2849    0    1]\n",
      " [2861    0    1]\n",
      " [2886    0    1]\n",
      " [2918    0    1]\n",
      " [2967    0    1]\n",
      " [2981    0    1]\n",
      " [3100    0    1]\n",
      " [3127    0    1]\n",
      " [3144    0    1]\n",
      " [3190    0    1]\n",
      " [3201    0    1]\n",
      " [3238    0    1]\n",
      " [3250    0    1]\n",
      " [3294    0    1]\n",
      " [3342    0    1]\n",
      " [3360    0    1]\n",
      " [3373    0    1]\n",
      " [3400    0    1]\n",
      " [3408    0    1]\n",
      " [3449    0    1]\n",
      " [3458    0    1]\n",
      " [3497    0    1]\n",
      " [3515    0    1]\n",
      " [3530    0    1]\n",
      " [3564    0    1]\n",
      " [3571    0    1]\n",
      " [3696    0    1]\n",
      " [3716    0    1]\n",
      " [3820    0    1]\n",
      " [3832    0    1]\n",
      " [3869    0    1]\n",
      " [3910    0    1]\n",
      " [3927    0    1]\n",
      " [3964    0    1]\n",
      " [4007    0    1]\n",
      " [4028    0    1]\n",
      " [4069    0    1]\n",
      " [4089    0    1]\n",
      " [4259    0    1]\n",
      " [4276    0    1]\n",
      " [4293    0    1]\n",
      " [4330    0    1]\n",
      " [4359    0    1]\n",
      " [4417    0    1]\n",
      " [4490    0    1]\n",
      " [4506    0    1]\n",
      " [4595    0    1]\n",
      " [4617    0    1]\n",
      " [4633    0    1]\n",
      " [4674    0    1]\n",
      " [4693    0    1]\n",
      " [4703    0    1]\n",
      " [4726    0    1]\n",
      " [4760    0    1]\n",
      " [4791    0    1]\n",
      " [4808    0    1]\n",
      " [4818    0    1]\n",
      " [4828    0    1]\n",
      " [4859    0    1]\n",
      " [4876    0    1]\n",
      " [4893    0    1]\n",
      " [4903    0    1]\n",
      " [4938    0    1]\n",
      " [4962    0    1]\n",
      " [4981    0    1]\n",
      " [5093    0    1]\n",
      " [5106    0    1]\n",
      " [5178    0    1]\n",
      " [5190    0    1]\n",
      " [5243    0    1]\n",
      " [5250    0    1]\n",
      " [5270    0    1]\n",
      " [5284    0    1]\n",
      " [5419    0    1]\n",
      " [5431    0    1]\n",
      " [5449    0    1]\n",
      " [5473    0    1]\n",
      " [5485    0    1]\n",
      " [5518    0    1]\n",
      " [5585    0    1]\n",
      " [5594    0    1]\n",
      " [5634    0    1]\n",
      " [5646    0    1]\n",
      " [5668    0    1]\n",
      " [5705    0    1]\n",
      " [5728    0    1]\n",
      " [5751    0    1]\n",
      " [5768    0    1]\n",
      " [5792    0    1]\n",
      " [5825    0    1]\n",
      " [5849    0    1]\n",
      " [5882    0    1]\n",
      " [5895    0    1]\n",
      " [5918    0    1]\n",
      " [5927    0    1]\n",
      " [5975    0    1]\n",
      " [5986    0    1]\n",
      " [6001    0    1]\n",
      " [6032    0    1]\n",
      " [6056    0    1]]\n",
      "<Epochs |  174 events (all good), -0.3 – 1.2 s, baseline off, ~11.9 MB, data loaded,\n",
      " '1': 174>\n",
      "subject_num= S13_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "tape_start_datapoints_npINT64= 853\n",
      "abs_wOnsets_dta_ndarray= [ 857.6     909.2721  931.4543  978.6929  988.2925 1014.6327 1084.0749\n",
      " 1132.9918 1146.8712 1183.449  1201.4082 1218.4596 1263.6667 1280.4286\n",
      " 1292.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [ 857  909  931  978  988 1014 1084 1132 1146 1183 1201 1218 1263 1280\n",
      " 1292]\n",
      "[[ 857    0    1]\n",
      " [ 909    0    1]\n",
      " [ 931    0    1]\n",
      " [ 978    0    1]\n",
      " [ 988    0    1]\n",
      " [1014    0    1]\n",
      " [1084    0    1]\n",
      " [1132    0    1]\n",
      " [1146    0    1]\n",
      " [1183    0    1]\n",
      " [1201    0    1]\n",
      " [1218    0    1]\n",
      " [1263    0    1]\n",
      " [1280    0    1]\n",
      " [1292    0    1]\n",
      " [1331    0    1]\n",
      " [1343    0    1]\n",
      " [1356    0    1]\n",
      " [1396    0    1]\n",
      " [1442    0    1]\n",
      " [1458    0    1]\n",
      " [1567    0    1]\n",
      " [1591    0    1]\n",
      " [1602    0    1]\n",
      " [1648    0    1]\n",
      " [1679    0    1]\n",
      " [1709    0    1]\n",
      " [1730    0    1]\n",
      " [1741    0    1]\n",
      " [1776    0    1]\n",
      " [1784    0    1]\n",
      " [1823    0    1]\n",
      " [1849    0    1]\n",
      " [1934    0    1]\n",
      " [1945    0    1]\n",
      " [1954    0    1]\n",
      " [1982    0    1]\n",
      " [1995    0    1]\n",
      " [2045    0    1]\n",
      " [2056    0    1]\n",
      " [2141    0    1]\n",
      " [2154    0    1]\n",
      " [2230    0    1]\n",
      " [2247    0    1]\n",
      " [2264    0    1]\n",
      " [2275    0    1]\n",
      " [2293    0    1]\n",
      " [2325    0    1]\n",
      " [2335    0    1]\n",
      " [2356    0    1]\n",
      " [2396    0    1]\n",
      " [2437    0    1]\n",
      " [2476    0    1]\n",
      " [2509    0    1]\n",
      " [2563    0    1]\n",
      " [2579    0    1]\n",
      " [2805    0    1]\n",
      " [2825    0    1]\n",
      " [2845    0    1]\n",
      " [2870    0    1]\n",
      " [2945    0    1]\n",
      " [2955    0    1]\n",
      " [2979    0    1]\n",
      " [3003    0    1]\n",
      " [3069    0    1]\n",
      " [3094    0    1]\n",
      " [3117    0    1]\n",
      " [3126    0    1]\n",
      " [3145    0    1]\n",
      " [3191    0    1]\n",
      " [3204    0    1]\n",
      " [3212    0    1]\n",
      " [3242    0    1]\n",
      " [3269    0    1]\n",
      " [3289    0    1]\n",
      " [3301    0    1]\n",
      " [3326    0    1]\n",
      " [3358    0    1]\n",
      " [3407    0    1]\n",
      " [3421    0    1]\n",
      " [3540    0    1]\n",
      " [3567    0    1]\n",
      " [3584    0    1]\n",
      " [3630    0    1]\n",
      " [3641    0    1]\n",
      " [3678    0    1]\n",
      " [3690    0    1]\n",
      " [3734    0    1]\n",
      " [3782    0    1]\n",
      " [3800    0    1]\n",
      " [3813    0    1]\n",
      " [3840    0    1]\n",
      " [3848    0    1]\n",
      " [3889    0    1]\n",
      " [3898    0    1]\n",
      " [3937    0    1]\n",
      " [3955    0    1]\n",
      " [3970    0    1]\n",
      " [4004    0    1]\n",
      " [4011    0    1]\n",
      " [4136    0    1]\n",
      " [4156    0    1]\n",
      " [4260    0    1]\n",
      " [4272    0    1]\n",
      " [4309    0    1]\n",
      " [4350    0    1]\n",
      " [4367    0    1]\n",
      " [4404    0    1]\n",
      " [4447    0    1]\n",
      " [4468    0    1]\n",
      " [4509    0    1]\n",
      " [4529    0    1]\n",
      " [4699    0    1]\n",
      " [4716    0    1]\n",
      " [4733    0    1]\n",
      " [4770    0    1]\n",
      " [4799    0    1]\n",
      " [4857    0    1]\n",
      " [4930    0    1]\n",
      " [4946    0    1]\n",
      " [5035    0    1]\n",
      " [5057    0    1]\n",
      " [5073    0    1]\n",
      " [5114    0    1]\n",
      " [5133    0    1]\n",
      " [5143    0    1]\n",
      " [5166    0    1]\n",
      " [5200    0    1]\n",
      " [5231    0    1]\n",
      " [5248    0    1]\n",
      " [5258    0    1]\n",
      " [5268    0    1]\n",
      " [5299    0    1]\n",
      " [5316    0    1]\n",
      " [5333    0    1]\n",
      " [5343    0    1]\n",
      " [5378    0    1]\n",
      " [5402    0    1]\n",
      " [5421    0    1]\n",
      " [5533    0    1]\n",
      " [5546    0    1]\n",
      " [5618    0    1]\n",
      " [5630    0    1]\n",
      " [5683    0    1]\n",
      " [5690    0    1]\n",
      " [5710    0    1]\n",
      " [5724    0    1]\n",
      " [5859    0    1]\n",
      " [5871    0    1]\n",
      " [5889    0    1]\n",
      " [5913    0    1]\n",
      " [5925    0    1]\n",
      " [5958    0    1]\n",
      " [6025    0    1]\n",
      " [6034    0    1]\n",
      " [6074    0    1]\n",
      " [6086    0    1]\n",
      " [6108    0    1]\n",
      " [6145    0    1]\n",
      " [6168    0    1]\n",
      " [6191    0    1]\n",
      " [6208    0    1]\n",
      " [6232    0    1]\n",
      " [6265    0    1]\n",
      " [6289    0    1]\n",
      " [6322    0    1]\n",
      " [6335    0    1]\n",
      " [6358    0    1]\n",
      " [6367    0    1]\n",
      " [6415    0    1]\n",
      " [6426    0    1]\n",
      " [6441    0    1]\n",
      " [6472    0    1]\n",
      " [6496    0    1]]\n",
      "<Epochs |  174 events (all good), -0.3 – 1.2 s, baseline off, ~11.9 MB, data loaded,\n",
      " '1': 174>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tape_word_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m     tape_perTape_epochs \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mEpochs(raw, events\u001b[38;5;241m=\u001b[39mwOnset_perTape_events, event_id\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, tmin\u001b[38;5;241m=\u001b[39mtmin, tmax\u001b[38;5;241m=\u001b[39mtmax, baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, preload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tape_perTape_epochs)\n\u001b[0;32m---> 56\u001b[0m all_tapes_epochs\u001b[38;5;241m.\u001b[39mappend(tape_word_epochs)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mfor i, stimulus_idx in enumerate(trial_indexes):\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    # We find the start of the tape in the raw EEG\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m# eeg_data = ds_all_words['eeg'].get_data()\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tape_word_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## (Not using this version) Trying the old way (Tape order 1st then subject cut 2nd)##\n",
    "\n",
    "# epochs parameters\n",
    "# Define your epoch window\n",
    "# We use a 1.5s window (including buffers for HHSA)\n",
    "tmin, tmax = -0.3, 1.2 \n",
    "\n",
    "# Get the actual word onset based on EEG triggers datapoints\n",
    "for i, stimulus_idx in enumerate(trial_indexes):\n",
    "    # Find the word onset time based on the segment sequence    \n",
    "    wOnset_perTape_DF = word_onset_essentials_DF.loc[word_onset_essentials_DF[\"Segment\"] == i+1, :] #.to_numpy()\n",
    "    #wOnset_time_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy() #*raw_sfreq\n",
    "    wOnset_datapoints_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy()*raw_sfreq\n",
    "    print(\"wOnset_datapoints_ndarray=\", wOnset_datapoints_ndarray[0:15], type(wOnset_datapoints_ndarray))\n",
    "\n",
    "    for subject in SUBJECTS[0:3]:\n",
    "        print(\"subject_num=\", subject)\n",
    "        print(\"tape_num=\", i+1)\n",
    "        \n",
    "        # 1. Load Raw as an Eelbrain-compatible object\n",
    "        raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)\n",
    "        raw_sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # 2. Get the events for the 12 tapes\n",
    "        events_DICT = eelbrain.load.mne.events(raw)\n",
    "\n",
    "        # Get the tape start time\n",
    "        tape_start_datapoints_npINT64 = events_DICT[i]['i_start']\n",
    "        #tape_start_time_npFLOAT64 = tape_start_datapoints_npINT64 / raw_sfreq\n",
    "        print(\"tape_start_datapoints_npINT64=\", tape_start_datapoints_npINT64)\n",
    "        #print(\"tape_start_time_npFLOAT64=\", tape_start_time_npFLOAT64, type(tape_start_time_npFLOAT64))\n",
    "\n",
    "        # Get the actual word onset time by the triggers\n",
    "        #absolute_onsets_time_ndarray = tape_start_time_npFLOAT64 + wOnset_time_ndarray\n",
    "        #print(\"absolute_onsets_time_ndarray=\", absolute_onsets_time_ndarray[0:15])\n",
    "        abs_wOnsets_dta_ndarray = tape_start_datapoints_npINT64 + wOnset_datapoints_ndarray\n",
    "        print(\"abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "        \n",
    "        # To exclude the decimal but leave the integer along, and turn FLOAT into INT\n",
    "        abs_wOnsets_dta_ndarray = np.trunc(abs_wOnsets_dta_ndarray).astype(int)\n",
    "        print(\"rounded_abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "\n",
    "        # Make epochs\n",
    "        # Create the empty (N, 3) event matrix based on wOnset per tape\n",
    "        wOnset_events = len(abs_wOnsets_dta_ndarray)\n",
    "        wOnset_perTape_events = np.zeros((wOnset_events, 3), dtype=int)\n",
    "        # Fill the columns\n",
    "        wOnset_perTape_events[:, 0] = abs_wOnsets_dta_ndarray  # Column 0: The sample indices\n",
    "        wOnset_perTape_events[:, 2] = i+1           # Column 2: The event ID (e.g., 1)\n",
    "        print(wOnset_perTape_events)\n",
    "\n",
    "        \n",
    "        #word_perTape_epochs = mne.epochs(raw, tmin=tmin, tmax=tmax, baseline=None, events=wOnset_perTape_events)\n",
    "        tape_perTape_epochs = mne.Epochs(raw, events=wOnset_perTape_events, event_id=i+1, tmin=tmin, tmax=tmax, baseline=None, preload=True)\n",
    "        print(tape_perTape_epochs)\n",
    "    all_tapes_epochs.append(tape_word_epochs)\n",
    "\"\"\"        \n",
    "    \n",
    "\n",
    "    \"\"\"(Gemini-Produced)\n",
    "    for i, stimulus_idx in enumerate(trial_indexes):\n",
    "        # We find the start of the tape in the raw EEG\n",
    "        tape_start = events[i]['i_start']#['time']\n",
    "        print([i], tape_start, type(tape_start))\n",
    "        \n",
    "        # Get word onset times for this specific tape from your list\n",
    "        # word_onsets[stimulus_idx] is your impulse predictor\n",
    "        onsets = word_onsets[stimulus_idx].time.times[word_onsets[stimulus_idx].x > 0]\n",
    "        print(onsets, len(onsets))\n",
    "    \n",
    "        \n",
    "        # Convert relative word times to absolute EEG times\n",
    "        absolute_onsets = tape_start + onsets\n",
    "        \n",
    "        # Create segments (Epochs) directly in Eelbrain\n",
    "        # This is much faster and more accurate than manual padding/cropping\n",
    "        tape_word_epochs = mne.epochs(\n",
    "            raw, tmin=tmin, tmax=tmax, baseline=None, \n",
    "            events=absolute_onsets\n",
    "        )\n",
    "        all_tapes_epochs.append(tape_word_epochs)\n",
    "    \n",
    "    # 5. Combine all words from all tapes into one Dataset\n",
    "    # This 'ds' will have a column for 'EEG' and can have a column for 'Subject'\n",
    "    ds_all_words = eelbrain.combine(all_tapes_epochs)\n",
    "    \n",
    "    # Now you can access the data for HHSA:\n",
    "    # eeg_data = ds_all_words['eeg'].get_data()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a31e8146-2195-473f-82b8-cec8f4056a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stimuli\n",
    "# ------------\n",
    "# Make sure to name the stimuli so that the TRFs can later be distinguished\n",
    "# Load the gammatone-spectrograms; use the time axis of these as reference\n",
    "gammatone = [eelbrain.load.unpickle(PREDICTOR_audio_DIR / f'{stimulus}~gammatone-8.pickle') for stimulus in STIMULI]\n",
    "\n",
    "# Resample the spectrograms to 100 Hz (time-step = 0.01 s), which we will use for TRFs\n",
    "gammatone = [x.bin(0.01, dim='time', label='start') for x in gammatone]\n",
    "\n",
    "# Pad onset with 100 ms and offset with 1 second; make sure to give the predictor a unique name as that will make it easier to identify the TRF later\n",
    "gammatone = [eelbrain.pad(x, tstart=-0.100, tstop=x.time.tstop + 1, name='gammatone') for x in gammatone]\n",
    "\n",
    "# Load the broad-band envelope and process it in the same way\n",
    "envelope = [eelbrain.load.unpickle(PREDICTOR_audio_DIR / f'{stimulus}~gammatone-1.pickle') for stimulus in STIMULI]  # Load in the data\n",
    "envelope = [x.bin(0.01, dim='time', label='start') for x in envelope]\n",
    "envelope = [eelbrain.pad(x, tstart=-0.100, tstop=x.time.tstop + 1, name='envelope') for x in envelope]\n",
    "onset_envelope = [eelbrain.load.unpickle(PREDICTOR_audio_DIR / f'{stimulus}~gammatone-on-1.pickle') for stimulus in STIMULI]\n",
    "onset_envelope = [x.bin(0.01, dim='time', label='start') for x in onset_envelope]\n",
    "onset_envelope = [eelbrain.pad(x, tstart=-0.100, tstop=x.time.tstop + 1, name='onset') for x in onset_envelope]\n",
    "\n",
    "# Load onset spectrograms and make sure the time dimension is equal to the gammatone spectrograms\n",
    "gammatone_onsets = [eelbrain.load.unpickle(PREDICTOR_audio_DIR / f'{stimulus}~gammatone-on-8.pickle') for stimulus in STIMULI]\n",
    "gammatone_onsets = [x.bin(0.01, dim='time', label='start') for x in gammatone_onsets]\n",
    "gammatone_onsets = [eelbrain.set_time(x, gt.time, name='gammatone_on') for x, gt in zip(gammatone_onsets, gammatone)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6973146b-a3bb-45c9-b959-46dfd49466ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_onsets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m     tape_start \u001b[38;5;241m=\u001b[39m events[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_start\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;66;03m#['time']\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Get word onset times for this specific tape from your list\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# word_onsets[stimulus_idx] is your impulse predictor\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     onsets \u001b[38;5;241m=\u001b[39m word_onsets[stimulus_idx]\u001b[38;5;241m.\u001b[39mtime\u001b[38;5;241m.\u001b[39mtimes[word_onsets[stimulus_idx]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(onsets, \u001b[38;5;28mlen\u001b[39m(onsets))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    # Convert relative word times to absolute EEG times\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    absolute_onsets = tape_start + onsets\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m# eeg_data = ds_all_words['eeg'].get_data()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_onsets' is not defined"
     ]
    }
   ],
   "source": [
    "## Got to the wrong length of the word onset\n",
    "\n",
    "## Gemini-Produced\n",
    "## To segment the raw EEG based on word start time == word onset time ##\n",
    "# ... your loading code ...\n",
    "\n",
    "for subject in SUBJECTS:\n",
    "    # 1. Load Raw as an Eelbrain-compatible object\n",
    "    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)\n",
    "    raw_sfreq = raw.info['sfreq']\n",
    "    \n",
    "    # 2. Get the events for the 12 tapes\n",
    "    events = eelbrain.load.mne.events(raw)\n",
    "    trial_indexes = [STIMULI.index(stimulus) for stimulus in events['event']]\n",
    "    \n",
    "    # 3. Define your epoch window\n",
    "    # We use a 1.5s window (including buffers for HHSA)\n",
    "    tmin, tmax = -0.3, 1.2 \n",
    "    \n",
    "    # 4. Extract segments for each tape\n",
    "    # This creates a list of NDVars, each containing the epochs for one tape\n",
    "    all_tapes_epochs = []\n",
    "    \n",
    "    for i, stimulus_idx in enumerate(trial_indexes):\n",
    "        # We find the start of the tape in the raw EEG\n",
    "        tape_start = events[i]['i_start']#['time']\n",
    "        \n",
    "        # Get word onset times for this specific tape from your list\n",
    "        # word_onsets[stimulus_idx] is your impulse predictor\n",
    "        onsets = word_onsets[stimulus_idx].time.times[word_onsets[stimulus_idx].x > 0]\n",
    "        print(onsets, len(onsets))\n",
    "\n",
    "        \"\"\"\n",
    "        # Convert relative word times to absolute EEG times\n",
    "        absolute_onsets = tape_start + onsets\n",
    "        \n",
    "        # Create segments (Epochs) directly in Eelbrain\n",
    "        # This is much faster and more accurate than manual padding/cropping\n",
    "        tape_word_epochs = mne.epochs(\n",
    "            raw, tmin=tmin, tmax=tmax, baseline=None, \n",
    "            events=absolute_onsets\n",
    "        )\n",
    "        all_tapes_epochs.append(tape_word_epochs)\n",
    "\n",
    "    # 5. Combine all words from all tapes into one Dataset\n",
    "    # This 'ds' will have a column for 'EEG' and can have a column for 'Subject'\n",
    "    ds_all_words = eelbrain.combine(all_tapes_epochs)\n",
    "    \n",
    "    # Now you can access the data for HHSA:\n",
    "    # eeg_data = ds_all_words['eeg'].get_data()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905b472f-1749-48cb-8b3e-85b9c14126d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== WORD ONSET IS DOWN BELOW =====\n",
      "<NDVar 'word': 500 time>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Estimate TRFs\\n# -------------\\n# Loop through subjects to estimate TRFs\\nfor subject in SUBJECTS:\\n    subject_trf_dir = TRF_DIR / subject[:3]\\n    subject_trf_dir.mkdir(exist_ok=True)\\n    # Generate all TRF paths so we can check whether any new TRFs need to be estimated\\n    trf_paths = {model: subject_trf_dir / f'{subject[:3]} {model}.pickle' for model in models}\\n    # Skip this subject if all files already exist\\n    #if all(path.exists() for path in trf_paths.values()):\\n        #continue\\n    # Load the EEG data\\n    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)  # subject /\\n    # Band-pass filter the raw data between 0.5 and 20 Hz\\n    raw.filter(0.5, 20)\\n    # Interpolate bad channels\\n    raw.interpolate_bads()\\n    # Extract the events marking the stimulus presentation from the EEG file\\n    events = eelbrain.load.fiff.events(raw)\\n    # Not all subjects have all trials; determine which stimuli are present\\n    trial_indexes = [STIMULI.index(stimulus) for stimulus in events['event']]\\n    # Extract the EEG data segments corresponding to the stimuli\\n    trial_durations = [durations[i] for i in trial_indexes]\\n    eeg = eelbrain.load.fiff.variable_length_epochs(events, -0.100, trial_durations, connectivity='auto')  #, decim=5 #decim=5 meaning to resample to sfreq=100Hz\\n    # Since trials are of unequal length, we will concatenate them for the TRF estimation.\\n    eeg_concatenated = eelbrain.concatenate(eeg)\\n    \\n    pprint(models.items)\\n    \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the time point of every word onset time according to the Alice csv file ##\n",
    "\n",
    "# Load word tables and convert tables into continuous time-series with matching time dimension\n",
    "word_tables = [eelbrain.load.unpickle(PREDICTOR_word_DIR / f'{stimulus}~word.pickle') for stimulus in STIMULI]\n",
    "word_onsets = [eelbrain.event_impulse_predictor(gt.time, ds=ds, name='word') for gt, ds in zip(gammatone, word_tables)]\n",
    "\n",
    "#print(word_tables[-1]) #  the onset & offset of each tape\n",
    "print(\"===== WORD ONSET IS DOWN BELOW =====\")\n",
    "print(word_onsets[0][0:5])\n",
    "# This is the original Durations of 12 tapes based on gammatone\n",
    "# Extract the duration of the stimuli, so we can later match the EEG to the stimuli\n",
    "durations = [gt.time.tmax for stimulus, gt in zip(STIMULI, gammatone)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Estimate TRFs\n",
    "# -------------\n",
    "# Loop through subjects to estimate TRFs\n",
    "for subject in SUBJECTS:\n",
    "    subject_trf_dir = TRF_DIR / subject[:3]\n",
    "    subject_trf_dir.mkdir(exist_ok=True)\n",
    "    # Generate all TRF paths so we can check whether any new TRFs need to be estimated\n",
    "    trf_paths = {model: subject_trf_dir / f'{subject[:3]} {model}.pickle' for model in models}\n",
    "    # Skip this subject if all files already exist\n",
    "    #if all(path.exists() for path in trf_paths.values()):\n",
    "        #continue\n",
    "    # Load the EEG data\n",
    "    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)  # subject /\n",
    "    # Band-pass filter the raw data between 0.5 and 20 Hz\n",
    "    raw.filter(0.5, 20)\n",
    "    # Interpolate bad channels\n",
    "    raw.interpolate_bads()\n",
    "    # Extract the events marking the stimulus presentation from the EEG file\n",
    "    events = eelbrain.load.fiff.events(raw)\n",
    "    # Not all subjects have all trials; determine which stimuli are present\n",
    "    trial_indexes = [STIMULI.index(stimulus) for stimulus in events['event']]\n",
    "    # Extract the EEG data segments corresponding to the stimuli\n",
    "    trial_durations = [durations[i] for i in trial_indexes]\n",
    "    eeg = eelbrain.load.fiff.variable_length_epochs(events, -0.100, trial_durations, connectivity='auto')  #, decim=5 #decim=5 meaning to resample to sfreq=100Hz\n",
    "    # Since trials are of unequal length, we will concatenate them for the TRF estimation.\n",
    "    eeg_concatenated = eelbrain.concatenate(eeg)\n",
    "    \n",
    "    pprint(models.items)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f36ef5-03af-4e7e-84fa-68e3009bf109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import eelbrain\n",
    "from pathlib import Path\n",
    "import re\n",
    "import emd  # Ensure you ran: pip install EMD-signal\n",
    "from scipy.signal import hilbert\n",
    "\"\"\"\n",
    "\n",
    "## Conduct HHSA ##\n",
    "## Step one\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. CORE ANALYSIS FUNCTIONS\n",
    "# ==========================================\n",
    "def get_inst_freq_amp(signal, fs):\n",
    "    \"\"\"\n",
    "    Calculates instantaneous amplitude and frequency using Hilbert Transform.\n",
    "    \"\"\"\n",
    "    analytic_signal = hilbert(signal)\n",
    "    amplitude = np.abs(analytic_signal)\n",
    "    instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
    "    \n",
    "    # Derivative of phase for frequency\n",
    "    inst_freq = np.diff(instantaneous_phase) / (2.0 * np.pi) * fs\n",
    "    # Pad last point to match length\n",
    "    inst_freq = np.append(inst_freq, inst_freq[-1])\n",
    "    \n",
    "    return amplitude, inst_freq\n",
    "\n",
    "def run_hhsa(signal, fs):\n",
    "    \"\"\"\n",
    "    Performs Two-Layer EMD with Mirror Padding for short signals.\n",
    "    \"\"\"\n",
    "    if signal.ndim > 1: signal = signal.flatten()\n",
    "    \n",
    "    n_samples = len(signal)\n",
    "    \n",
    "    # Safety Check for empty/tiny signals\n",
    "    if n_samples < 10: return None\n",
    "\n",
    "    # --- A. MIRROR PADDING (Crucial for 114-point TRFs) ---\n",
    "    # We reflect the signal to make it 3x longer so EMD works\n",
    "    pad_width = n_samples\n",
    "    padded_signal = np.pad(signal, pad_width, mode='reflect')\n",
    "    \n",
    "    # --- B. LAYER 1: CARRIER DECOMPOSITION ---\n",
    "    try:\n",
    "        # Use emd.sift.sift (Quinn library)\n",
    "        # max_imfs=5 is enough for a simple TRF response\n",
    "        imfs_layer1 = emd.sift.sift(padded_signal, max_imfs=5)\n",
    "        imfs_layer1 = imfs_layer1.T  # Transpose to (N_IMFs, N_Time)\n",
    "    except Exception:\n",
    "        return None\n",
    "        \n",
    "    holo_points = []\n",
    "    \n",
    "    for imf_c in imfs_layer1:\n",
    "        # Un-pad: Extract the middle 'real' part\n",
    "        imf_c_real = imf_c[pad_width : pad_width + n_samples]\n",
    "        \n",
    "        # Get Carrier Frequency & Envelope\n",
    "        env_c, freq_c = get_inst_freq_amp(imf_c_real, fs)\n",
    "        \n",
    "        # Skip flat/empty IMFs\n",
    "        if np.sum(np.abs(env_c)) < 1e-10: continue\n",
    "\n",
    "        # --- C. LAYER 2: AM DECOMPOSITION ---\n",
    "        # Pad the envelope before sifting\n",
    "        padded_env = np.pad(env_c, pad_width, mode='reflect')\n",
    "        \n",
    "        try:\n",
    "            imfs_layer2 = emd.sift.sift(padded_env, max_imfs=5)\n",
    "            imfs_layer2 = imfs_layer2.T\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        for imf_am in imfs_layer2:\n",
    "            # Un-pad AM result\n",
    "            imf_am_real = imf_am[pad_width : pad_width + n_samples]\n",
    "            \n",
    "            _, freq_am = get_inst_freq_amp(imf_am_real, fs)\n",
    "            power_am = imf_am_real**2\n",
    "            \n",
    "            # Filter for valid graph range\n",
    "            mask = (freq_c > 0) & (freq_c < fs/2) & (freq_am > 0) & (freq_am < fs/2)\n",
    "            idx = np.where(mask)[0]\n",
    "            \n",
    "            if len(idx) > 0:\n",
    "                # Store [Carrier_Freq, AM_Freq, Power]\n",
    "                points = np.vstack((freq_c[idx], freq_am[idx], power_am[idx])).T\n",
    "                holo_points.append(points)\n",
    "                \n",
    "    if not holo_points: return None\n",
    "    return np.vstack(holo_points)\n",
    "\n",
    "# ==========================================\n",
    "# 2. EXPERIMENT CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_ROOT = Path(\"/Users/neuroling/Downloads/DINGHSIN_Results/Alice_Experiments_Results\")\n",
    "EEG_DIR = DATA_ROOT / 'EEG_ESLs' / 'Alice_ESL_ICAed_fif'\n",
    "TRF_DIR = DATA_ROOT / 'TRFs_ESLs'\n",
    "\n",
    "# Extract Subjects\n",
    "ESL_SUBJECTS = [path.name for path in EEG_DIR.iterdir() if re.match(r'n_2_S\\d*', path.name)]\n",
    "print(f\"Found {len(ESL_SUBJECTS)} subjects.\")\n",
    "\n",
    "# Settings\n",
    "TARGET_MODEL = 'Fzero'\n",
    "LIMIT_CARRIER = 5  # Hz\n",
    "LIMIT_AM = 5       # Hz\n",
    "NBINS = 50\n",
    "\n",
    "group_spectrum_sum = None\n",
    "n_subjects_processed = 0\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN LOOP\n",
    "# ==========================================\n",
    "print(f\"Starting Group HHSA on Model: {TARGET_MODEL}\")\n",
    "\n",
    "for subject in ESL_SUBJECTS:\n",
    "    subject_id_short = subject[4:8] # 'S010'\n",
    "    file_path = TRF_DIR / subject_id_short / f'{subject_id_short} {TARGET_MODEL}.pickle'\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # --- LOAD DATA ---\n",
    "        trf_obj = eelbrain.load.unpickle(file_path)\n",
    "        \n",
    "        # Handle h vs h_scaled\n",
    "        if hasattr(trf_obj, 'h_scaled'):\n",
    "            data_ndvar = trf_obj.h_scaled\n",
    "        else:\n",
    "            data_ndvar = trf_obj.h\n",
    "\n",
    "        # Handle Tuple (Partitioned Data)\n",
    "        if isinstance(data_ndvar, tuple):\n",
    "            data_ndvar = data_ndvar[0]\n",
    "\n",
    "        # --- EXTRACT PREDICTOR ---\n",
    "        trf_final = None\n",
    "        \n",
    "        # Strategy 1: Try name 'envelope'\n",
    "        try:\n",
    "            trf_final = data_ndvar['Fzero']\n",
    "        except:\n",
    "            # Strategy 2: Try Index\n",
    "            # If dims are [predictor, time], we grab the 2nd predictor (index 1)\n",
    "            # Assuming the order is [F0, Envelope]\n",
    "            dims = data_ndvar.dimnames\n",
    "            non_time_dims = [d for d in dims if d != 'time' and d != 'sensor']\n",
    "            \n",
    "            if len(non_time_dims) > 0:\n",
    "                dim_name = non_time_dims[0]\n",
    "                # Index 1 = Envelope. Change to 0 if you want F0.\n",
    "                trf_final = data_ndvar.sub(**{dim_name: 1}) \n",
    "            else:\n",
    "                trf_final = data_ndvar\n",
    "\n",
    "        # Average Sensors\n",
    "        if 'sensor' in trf_final.dimnames:\n",
    "            trf_final = trf_final.mean('sensor')\n",
    "            \n",
    "        trf_vector = trf_final.x\n",
    "        fs = 1.0 / trf_final.time.tstep\n",
    "        \n",
    "        # --- RUN HHSA ---\n",
    "        # print(f\"  Processing {subject_id_short}...\")\n",
    "        holo_data = run_hhsa(trf_vector, fs)\n",
    "        \n",
    "        if holo_data is None: \n",
    "            continue\n",
    "            \n",
    "        # --- DEBUG PRINT ---\n",
    "        # Print the average frequencies found for this subject\n",
    "        avg_fc = np.mean(holo_data[:, 0])\n",
    "        avg_fam = np.mean(holo_data[:, 1])\n",
    "        print(f\"  Subject {subject_id_short}: Avg Carrier={avg_fc:.2f}Hz, Avg AM={avg_fam:.2f}Hz\")\n",
    "        # -------------------\n",
    "        \n",
    "\n",
    "        # --- BIN RESULTS ---\n",
    "        fc = holo_data[:, 0]\n",
    "        fam = holo_data[:, 1]\n",
    "        power = holo_data[:, 2]\n",
    "        \n",
    "        H_subj, xedges, yedges = np.histogram2d(fc, fam, bins=NBINS, weights=power,\n",
    "                                           range=[[0, LIMIT_CARRIER], [0, LIMIT_AM]])\n",
    "        \n",
    "        if group_spectrum_sum is None:\n",
    "            group_spectrum_sum = H_subj\n",
    "        else:\n",
    "            group_spectrum_sum += H_subj\n",
    "            \n",
    "        n_subjects_processed += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Failed on {subject_id_short}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ==========================================\n",
    "# 4. PLOT FINAL RESULT\n",
    "# ==========================================\n",
    "if n_subjects_processed > 0:\n",
    "    group_avg_spectrum = group_spectrum_sum / n_subjects_processed\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Note: .T is used because imshow expects [rows, cols] = [y, x]\n",
    "    plt.imshow(group_avg_spectrum.T, origin='lower', cmap='jet', aspect='auto',\n",
    "               extent=[0, LIMIT_CARRIER, 0, LIMIT_AM], interpolation='gaussian')\n",
    "    \n",
    "    plt.colorbar(label='Modulation Power (a.u.)')\n",
    "    plt.xlabel('Carrier Frequency (Hz)')\n",
    "    plt.ylabel('AM Frequency (Hz)')\n",
    "    plt.title(f'Group HHSA (N={n_subjects_processed})\\nModel: {TARGET_MODEL} | Predictor: Envelope')\n",
    "    \n",
    "    # Diagonal line (1:1 coupling)\n",
    "    plt.plot([0, LIMIT_AM], [0, LIMIT_AM], 'w--', alpha=0.5)\n",
    "    plt.savefig(TRF_DIR / 'ESLs_Fzero_HHSA_TRF.png')\n",
    "    plt.show()\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"No subjects processed. Please check if pickle files contain the expected data structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ada7a-18b9-46c9-9480-435c3278c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import eelbrain\n",
    "from pathlib import Path\n",
    "import emd\n",
    "from scipy.signal import hilbert\n",
    "\"\"\"\n",
    "# Version 1 of IMFs in each layers (=second EMD)\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_ROOT = Path(\"/Users/neuroling/Downloads/DINGHSIN_Results/Alice_Experiments_Results\")\n",
    "TRF_DIR = DATA_ROOT / 'TRFs_ESLs'\n",
    "TARGET_MODEL = 'Fzero+envelope'\n",
    "TARGET_SUBJECT = 'S010'  # <--- Change this to look at different subjects\n",
    "\n",
    "# Which Layer 1 IMF do you want to decompose further?\n",
    "# 0 = First IMF (Fastest/Highest Freq), 1 = Second IMF, etc.\n",
    "TARGET_IMF_INDEX = 1 \n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD DATA (Robust Loading)\n",
    "# ==========================================\n",
    "file_path = TRF_DIR / TARGET_SUBJECT / f'{TARGET_SUBJECT} {TARGET_MODEL}.pickle'\n",
    "print(f\"Loading {TARGET_SUBJECT}...\")\n",
    "\n",
    "try:\n",
    "    trf_obj = eelbrain.load.unpickle(file_path)\n",
    "    \n",
    "    # Handle h vs h_scaled\n",
    "    if hasattr(trf_obj, 'h_scaled'):\n",
    "        data_ndvar = trf_obj.h_scaled\n",
    "    else:\n",
    "        data_ndvar = trf_obj.h\n",
    "        \n",
    "    # Handle Tuple\n",
    "    if isinstance(data_ndvar, tuple):\n",
    "        data_ndvar = data_ndvar[0]\n",
    "\n",
    "    # Extract Predictor (Strategy: Index)\n",
    "    # Assumes order is [F0, Envelope] -> Index 1 is Envelope\n",
    "    # If your model is just 'envelope', it might be Index 0.\n",
    "    try:\n",
    "        # Try finding 'envelope' by name\n",
    "        trf_final = data_ndvar['envelope']\n",
    "        pred_name = \"Envelope\"\n",
    "    except:\n",
    "        # Fallback to Index 1\n",
    "        dims = data_ndvar.dimnames\n",
    "        non_time_dims = [d for d in dims if d != 'time' and d != 'sensor']\n",
    "        if len(non_time_dims) > 0:\n",
    "            trf_final = data_ndvar.sub(**{non_time_dims[0]: 1})\n",
    "            pred_name = \"Predictor (Index 1)\"\n",
    "        else:\n",
    "            trf_final = data_ndvar\n",
    "            pred_name = \"Predictor\"\n",
    "\n",
    "    # Average Sensors\n",
    "    if 'sensor' in trf_final.dimnames:\n",
    "        trf_final = trf_final.mean('sensor')\n",
    "        \n",
    "    signal = trf_final.x\n",
    "    times = trf_final.time.times\n",
    "    fs = 1.0 / trf_final.time.tstep\n",
    "\n",
    "    if signal.ndim > 1: signal = signal.flatten()\n",
    "    print(f\"Data Loaded: {len(signal)} samples\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==========================================\n",
    "# 3. PROCESSING (With Mirror Padding)\n",
    "# ==========================================\n",
    "\n",
    "# --- A. Layer 1 Decomposition ---\n",
    "pad_width = len(signal)\n",
    "padded_signal = np.pad(signal, pad_width, mode='reflect')\n",
    "\n",
    "# Run EMD (Sift)\n",
    "imfs_layer1_padded = emd.sift.sift(padded_signal, max_imfs=5)\n",
    "\n",
    "# Un-pad Layer 1\n",
    "# Note: emd output is (Samples, IMFs)\n",
    "imfs_layer1 = imfs_layer1_padded[pad_width : pad_width + len(signal), :]\n",
    "n_imfs1 = imfs_layer1.shape[1]\n",
    "\n",
    "# --- B. Layer 2 Decomposition (Target IMF) ---\n",
    "# Extract the target IMF (e.g., IMF 0)\n",
    "target_imf_padded = imfs_layer1_padded[:, TARGET_IMF_INDEX]\n",
    "\n",
    "# Get Envelope (using Hilbert)\n",
    "analytic = hilbert(target_imf_padded)\n",
    "envelope_padded = np.abs(analytic)\n",
    "\n",
    "# Run EMD on Envelope\n",
    "imfs_layer2_padded = emd.sift.sift(envelope_padded, max_imfs=4)\n",
    "\n",
    "# Un-pad Layer 2\n",
    "envelope_real = envelope_padded[pad_width : pad_width + len(signal)]\n",
    "imfs_layer2 = imfs_layer2_padded[pad_width : pad_width + len(signal), :]\n",
    "n_imfs2 = imfs_layer2.shape[1]\n",
    "\n",
    "# ==========================================\n",
    "# 4. PLOTTING\n",
    "# ==========================================\n",
    "\n",
    "# --- FIGURE 1: LAYER 1 (Carrier) ---\n",
    "fig1, axes1 = plt.subplots(n_imfs1 + 1, 1, figsize=(10, 8), sharex=True)\n",
    "fig1.suptitle(f\"Layer 1: Carrier Decomposition ({TARGET_SUBJECT})\", fontsize=14)\n",
    "\n",
    "# Plot Original Signal\n",
    "axes1[0].plot(times, signal, 'k', label='Original TRF')\n",
    "axes1[0].set_title(f\"Original Signal: {pred_name}\")\n",
    "axes1[0].legend(loc='upper right')\n",
    "\n",
    "# Plot IMFs\n",
    "for i in range(n_imfs1):\n",
    "    ax = axes1[i + 1]\n",
    "    ax.plot(times, imfs_layer1[:, i], 'b')\n",
    "    ax.set_ylabel(f\"IMF {i+1}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes1[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(TRF_DIR / 'ESLs_S010_layer1-IMFs_HHSA_TRF.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- FIGURE 2: LAYER 2 (Amplitude Modulation) ---\n",
    "fig2, axes2 = plt.subplots(n_imfs2 + 1, 1, figsize=(10, 8), sharex=True)\n",
    "fig2.suptitle(f\"Layer 2: AM Decomposition of Carrier IMF {TARGET_IMF_INDEX+1}\", fontsize=14)\n",
    "\n",
    "# Plot Envelope\n",
    "axes2[0].plot(times, envelope_real, 'r', label=f'Envelope of IMF {TARGET_IMF_INDEX+1}')\n",
    "axes2[0].set_title(\"Amplitude Envelope (Input to Layer 2)\")\n",
    "axes2[0].legend(loc='upper right')\n",
    "\n",
    "# Plot AM IMFs\n",
    "for i in range(n_imfs2):\n",
    "    ax = axes2[i + 1]\n",
    "    ax.plot(times, imfs_layer2[:, i], 'g')\n",
    "    ax.set_ylabel(f\"AM IMF {i+1}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes2[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(TRF_DIR / 'ESLs_S010_layer2-IMF2_HHSA_TRF.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
