{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7237917c-db70-442b-97d1-8d04eaee9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3d1c9c24-9a0f-450f-8360-dcb4186d8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import eelbrain\n",
    "import mne\n",
    "#import trftools\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "import emd  # Ensure you ran: pip install EMD-signal\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "68a6da85-e566-4830-8c9d-40c93a8ef389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_2_S030_ICAed_raw.fif', 'n_2_S027_ICAed_raw.fif', 'n_2_S023_ICAed_raw.fif', 'n_2_S034_ICAed_raw.fif', 'n_2_S024_ICAed_raw.fif', 'n_2_S019_ICAed_raw.fif', 'n_2_S020_ICAed_raw.fif', 'n_2_S013_ICAed_raw.fif', 'n_2_S017_ICAed_raw.fif', 'n_2_S039_ICAed_raw.fif', 'n_2_S010_ICAed_raw.fif', 'n_2_S029_ICAed_raw.fif', 'n_2_S015_ICAed_raw.fif', 'n_2_S028_ICAed_raw.fif', 'n_2_S011_ICAed_raw.fif', 'n_2_S038_ICAed_raw.fif', 'n_2_S016_ICAed_raw.fif', 'n_2_S012_ICAed_raw.fif', 'n_2_S021_ICAed_raw.fif', 'n_2_S036_ICAed_raw.fif', 'n_2_S032_ICAed_raw.fif', 'n_2_S025_ICAed_raw.fif', 'n_2_S035_ICAed_raw.fif', 'n_2_S022_ICAed_raw.fif', 'n_2_S026_ICAed_raw.fif', 'n_2_S031_ICAed_raw.fif']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "## ESLs ##\n",
    "## Import the raw EEG data of ESLs(Alice)\n",
    "\n",
    "STIMULI = [str(i) for i in range(1, 13)]\n",
    "DATA_ROOT = Path(\"/Users/neuroling/Downloads/DINGHSIN_Results/Alice_Experiments_Results\")\n",
    "#DATA_ROOT = Path(\"/Volumes/Neurolang_1/Master Program/New_Thesis_topic/Experiments_Results\")  #Path(\"~\").expanduser() / 'Data' / 'Alice'\n",
    "PREDICTOR_audio_DIR = DATA_ROOT / 'TRFs_pridictors/audio_predictors'\n",
    "PREDICTOR_word_DIR = DATA_ROOT / 'TRFs_pridictors/word_predictors'\n",
    "EEG_DIR = DATA_ROOT / 'EEG_ESLs' / 'Alice_ESL_ICAed_fif'\n",
    "ESL_SUBJECTS = [path.name for path in EEG_DIR.iterdir() if re.match(r'n_2_S\\d*', path.name)]  #S01_alice-raw.fif\n",
    "# Define a target directory for TRF estimates and make sure the directory is created\n",
    "TRF_DIR = DATA_ROOT / 'TRFs_ESLs'\n",
    "TRF_DIR.mkdir(exist_ok=True)\n",
    "print(ESL_SUBJECTS)\n",
    "print(len(ESL_SUBJECTS))  # 26\n",
    "\n",
    "DST = TRF_DIR / 'ESLs_figures'\n",
    "DST.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4745a1e4-b314-4eef-8bcd-33744acd0daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S44_Alice-natives_sfreq-100_raw.fif', 'S20_Alice-natives_sfreq-100_raw.fif', 'S13_Alice-natives_sfreq-100_raw.fif', 'S01_Alice-natives_sfreq-100_raw.fif', 'S16_Alice-natives_sfreq-100_raw.fif', 'S41_Alice-natives_sfreq-100_raw.fif', 'S25_Alice-natives_sfreq-100_raw.fif', 'S37_Alice-natives_sfreq-100_raw.fif', 'S04_Alice-natives_sfreq-100_raw.fif', 'S18_Alice-natives_sfreq-100_raw.fif', 'S39_Alice-natives_sfreq-100_raw.fif', 'S10_Alice-natives_sfreq-100_raw.fif', 'S15_Alice-natives_sfreq-100_raw.fif', 'S26_Alice-natives_sfreq-100_raw.fif', 'S42_Alice-natives_sfreq-100_raw.fif', 'S34_Alice-natives_sfreq-100_raw.fif', 'S38_Alice-natives_sfreq-100_raw.fif', 'S19_Alice-natives_sfreq-100_raw.fif', 'S06_Alice-natives_sfreq-100_raw.fif', 'S35_Alice-natives_sfreq-100_raw.fif', 'S14_Alice-natives_sfreq-100_raw.fif', 'S03_Alice-natives_sfreq-100_raw.fif', 'S11_Alice-natives_sfreq-100_raw.fif', 'S22_Alice-natives_sfreq-100_raw.fif', 'S05_Alice-natives_sfreq-100_raw.fif', 'S36_Alice-natives_sfreq-100_raw.fif', 'S40_Alice-natives_sfreq-100_raw.fif', 'S17_Alice-natives_sfreq-100_raw.fif', 'S12_Alice-natives_sfreq-100_raw.fif', 'S45_Alice-natives_sfreq-100_raw.fif', 'S21_Alice-natives_sfreq-100_raw.fif', 'S48_Alice-natives_sfreq-100_raw.fif', 'S08_Alice-natives_sfreq-100_raw.fif']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "## Natives ##\n",
    "## Import the raw EEG data of ESLs(Alice)\n",
    "\n",
    "STIMULI = [str(i) for i in range(1, 13)]\n",
    "DATA_ROOT = Path(\"/Users/neuroling/Downloads/DINGHSIN_Results/Alice_Experiments_Results\") #Path(\"/Volumes/Neurolang_1/Master Program/New_Thesis_topic/Experiments_Results\")  #Path(\"~\").expanduser() / 'Data' / 'Alice'\n",
    "PREDICTOR_audio_DIR = DATA_ROOT / 'TRFs_pridictors/audio_predictors'\n",
    "PREDICTOR_word_DIR = DATA_ROOT / 'TRFs_pridictors/word_predictors'\n",
    "IMF_DIR = DATA_ROOT/ \"TRFs_pridictors/IF_predictors\"\n",
    "F0_DIR = DATA_ROOT/ \"TRFs_pridictors/F0_predictors\"\n",
    "IMFsLIST = [path.name for path in IMF_DIR.iterdir() if re.match(r'Alice_IF_IMF_*', path.name)]\n",
    "EEG_DIR = DATA_ROOT / 'EEG_Natives' / 'Alice_natives_ICAed_fif'\n",
    "SUBJECTS = [path.name for path in EEG_DIR.iterdir() if re.match(r'S\\d*', path.name[:4])]\n",
    "# Define a target directory for TRF estimates and make sure the directory is created\n",
    "TRF_DIR = DATA_ROOT / 'TRFs_Natives'\n",
    "TRF_DIR.mkdir(exist_ok=True)\n",
    "print(SUBJECTS)\n",
    "print(len(SUBJECTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "45be25b7-0a68-4d75-a6f6-1b70a57e1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "2\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "3\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "4\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "5\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "6\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "7\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "8\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "9\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "10\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "11\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n",
      "12\n",
      "         Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns] <class 'pandas.core.frame.DataFrame'>\n",
      "174     0.479840\n",
      "175     0.592424\n",
      "176     0.810806\n",
      "177     0.912952\n",
      "178     1.415810\n",
      "         ...    \n",
      "346    58.681660\n",
      "347    58.861252\n",
      "348    59.148599\n",
      "349    59.251158\n",
      "350    59.687374\n",
      "Name: onset, Length: 177, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import the csv data\n",
    "csv_data = DATA_ROOT / \"Alice(EEG_mat_and stimuli)\" / \"AliceChapterOne-EEG.csv\"  # self-made LMM data form\n",
    "\n",
    "\n",
    "word_onset_LIST = []\n",
    "with open(csv_data, \"r\", encoding=\"UTF-8\") as f:\n",
    "    fileDF = pd.read_csv(f, sep=\",\")\n",
    "    #print(fileDF.columns)\n",
    "    # word_onset = fileDF[\"onset\"]\n",
    "    # print(word_onset, type(word_onset))\n",
    "\n",
    "    word_onset_essentials_DF = fileDF.iloc[:,[0, 1, 2] ]     # first column\n",
    "    #print(word_onset_essentials_DF[\"onset\"], word_onset_essentials_DF[\"onset\"])\n",
    "    #print()\n",
    "\n",
    "    for i in range(1, 13):\n",
    "        print(i)\n",
    "        #w_S = word_onset_essentials_DF.loc[:, [i][\"Word\"]]\n",
    "        #wOnset_F = word_onset_essentials_DF.loc[:, 2]\n",
    "        #print(w_S)\n",
    "\n",
    "        wOnset_DF = word_onset_essentials_DF.loc[word_onset_essentials_DF[\"Segment\"] == 2, :]\n",
    "        print(wOnset_DF, type(wOnset_DF))\n",
    "        print(wOnset_DF[\"onset\"])\n",
    "        \"\"\"\n",
    "        if \n",
    "        w_S = word_onset_essentials_DF.iloc[i, 0]\n",
    "        wOnset_F = word_onset_essentials_DF.iloc[i, 2]\n",
    "        print(w_S, wOnset_F)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ef6d81d-35f7-4656-8353-4a451591a71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wOnset_datapoints_ndarray= [  4.6     56.2721  78.4543 125.6929 135.2925 161.6327 231.0749 279.9918\n",
      " 293.8712 330.449  348.4082 365.4596 410.6667 427.4286 439.8799] <class 'numpy.ndarray'>\n",
      "subject_num= S44_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "tape_start_datapoints_npINT64= 172\n",
      "abs_wOnsets_dta_ndarray= [176.6    228.2721 250.4543 297.6929 307.2925 333.6327 403.0749 451.9918\n",
      " 465.8712 502.449  520.4082 537.4596 582.6667 599.4286 611.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [176 228 250 297 307 333 403 451 465 502 520 537 582 599 611]\n",
      "[[ 176    0    1]\n",
      " [ 228    0    1]\n",
      " [ 250    0    1]\n",
      " [ 297    0    1]\n",
      " [ 307    0    1]\n",
      " [ 333    0    1]\n",
      " [ 403    0    1]\n",
      " [ 451    0    1]\n",
      " [ 465    0    1]\n",
      " [ 502    0    1]\n",
      " [ 520    0    1]\n",
      " [ 537    0    1]\n",
      " [ 582    0    1]\n",
      " [ 599    0    1]\n",
      " [ 611    0    1]\n",
      " [ 650    0    1]\n",
      " [ 662    0    1]\n",
      " [ 675    0    1]\n",
      " [ 715    0    1]\n",
      " [ 761    0    1]\n",
      " [ 777    0    1]\n",
      " [ 886    0    1]\n",
      " [ 910    0    1]\n",
      " [ 921    0    1]\n",
      " [ 967    0    1]\n",
      " [ 998    0    1]\n",
      " [1028    0    1]\n",
      " [1049    0    1]\n",
      " [1060    0    1]\n",
      " [1095    0    1]\n",
      " [1103    0    1]\n",
      " [1142    0    1]\n",
      " [1168    0    1]\n",
      " [1253    0    1]\n",
      " [1264    0    1]\n",
      " [1273    0    1]\n",
      " [1301    0    1]\n",
      " [1314    0    1]\n",
      " [1364    0    1]\n",
      " [1375    0    1]\n",
      " [1460    0    1]\n",
      " [1473    0    1]\n",
      " [1549    0    1]\n",
      " [1566    0    1]\n",
      " [1583    0    1]\n",
      " [1594    0    1]\n",
      " [1612    0    1]\n",
      " [1644    0    1]\n",
      " [1654    0    1]\n",
      " [1675    0    1]\n",
      " [1715    0    1]\n",
      " [1756    0    1]\n",
      " [1795    0    1]\n",
      " [1828    0    1]\n",
      " [1882    0    1]\n",
      " [1898    0    1]\n",
      " [2124    0    1]\n",
      " [2144    0    1]\n",
      " [2164    0    1]\n",
      " [2189    0    1]\n",
      " [2264    0    1]\n",
      " [2274    0    1]\n",
      " [2298    0    1]\n",
      " [2322    0    1]\n",
      " [2388    0    1]\n",
      " [2413    0    1]\n",
      " [2436    0    1]\n",
      " [2445    0    1]\n",
      " [2464    0    1]\n",
      " [2510    0    1]\n",
      " [2523    0    1]\n",
      " [2531    0    1]\n",
      " [2561    0    1]\n",
      " [2588    0    1]\n",
      " [2608    0    1]\n",
      " [2620    0    1]\n",
      " [2645    0    1]\n",
      " [2677    0    1]\n",
      " [2726    0    1]\n",
      " [2740    0    1]\n",
      " [2859    0    1]\n",
      " [2886    0    1]\n",
      " [2903    0    1]\n",
      " [2949    0    1]\n",
      " [2960    0    1]\n",
      " [2997    0    1]\n",
      " [3009    0    1]\n",
      " [3053    0    1]\n",
      " [3101    0    1]\n",
      " [3119    0    1]\n",
      " [3132    0    1]\n",
      " [3159    0    1]\n",
      " [3167    0    1]\n",
      " [3208    0    1]\n",
      " [3217    0    1]\n",
      " [3256    0    1]\n",
      " [3274    0    1]\n",
      " [3289    0    1]\n",
      " [3323    0    1]\n",
      " [3330    0    1]\n",
      " [3455    0    1]\n",
      " [3475    0    1]\n",
      " [3579    0    1]\n",
      " [3591    0    1]\n",
      " [3628    0    1]\n",
      " [3669    0    1]\n",
      " [3686    0    1]\n",
      " [3723    0    1]\n",
      " [3766    0    1]\n",
      " [3787    0    1]\n",
      " [3828    0    1]\n",
      " [3848    0    1]\n",
      " [4018    0    1]\n",
      " [4035    0    1]\n",
      " [4052    0    1]\n",
      " [4089    0    1]\n",
      " [4118    0    1]\n",
      " [4176    0    1]\n",
      " [4249    0    1]\n",
      " [4265    0    1]\n",
      " [4354    0    1]\n",
      " [4376    0    1]\n",
      " [4392    0    1]\n",
      " [4433    0    1]\n",
      " [4452    0    1]\n",
      " [4462    0    1]\n",
      " [4485    0    1]\n",
      " [4519    0    1]\n",
      " [4550    0    1]\n",
      " [4567    0    1]\n",
      " [4577    0    1]\n",
      " [4587    0    1]\n",
      " [4618    0    1]\n",
      " [4635    0    1]\n",
      " [4652    0    1]\n",
      " [4662    0    1]\n",
      " [4697    0    1]\n",
      " [4721    0    1]\n",
      " [4740    0    1]\n",
      " [4852    0    1]\n",
      " [4865    0    1]\n",
      " [4937    0    1]\n",
      " [4949    0    1]\n",
      " [5002    0    1]\n",
      " [5009    0    1]\n",
      " [5029    0    1]\n",
      " [5043    0    1]\n",
      " [5178    0    1]\n",
      " [5190    0    1]\n",
      " [5208    0    1]\n",
      " [5232    0    1]\n",
      " [5244    0    1]\n",
      " [5277    0    1]\n",
      " [5344    0    1]\n",
      " [5353    0    1]\n",
      " [5393    0    1]\n",
      " [5405    0    1]\n",
      " [5427    0    1]\n",
      " [5464    0    1]\n",
      " [5487    0    1]\n",
      " [5510    0    1]\n",
      " [5527    0    1]\n",
      " [5551    0    1]\n",
      " [5584    0    1]\n",
      " [5608    0    1]\n",
      " [5641    0    1]\n",
      " [5654    0    1]\n",
      " [5677    0    1]\n",
      " [5686    0    1]\n",
      " [5734    0    1]\n",
      " [5745    0    1]\n",
      " [5760    0    1]\n",
      " [5791    0    1]\n",
      " [5815    0    1]]\n",
      "<Epochs |  174 events (all good), -0.3 – 1.2 s, baseline off, ~11.9 MB, data loaded,\n",
      " '1': 174>\n",
      "subject_num= S20_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "tape_start_datapoints_npINT64= 413\n",
      "abs_wOnsets_dta_ndarray= [417.6    469.2721 491.4543 538.6929 548.2925 574.6327 644.0749 692.9918\n",
      " 706.8712 743.449  761.4082 778.4596 823.6667 840.4286 852.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [417 469 491 538 548 574 644 692 706 743 761 778 823 840 852]\n",
      "[[ 417    0    1]\n",
      " [ 469    0    1]\n",
      " [ 491    0    1]\n",
      " [ 538    0    1]\n",
      " [ 548    0    1]\n",
      " [ 574    0    1]\n",
      " [ 644    0    1]\n",
      " [ 692    0    1]\n",
      " [ 706    0    1]\n",
      " [ 743    0    1]\n",
      " [ 761    0    1]\n",
      " [ 778    0    1]\n",
      " [ 823    0    1]\n",
      " [ 840    0    1]\n",
      " [ 852    0    1]\n",
      " [ 891    0    1]\n",
      " [ 903    0    1]\n",
      " [ 916    0    1]\n",
      " [ 956    0    1]\n",
      " [1002    0    1]\n",
      " [1018    0    1]\n",
      " [1127    0    1]\n",
      " [1151    0    1]\n",
      " [1162    0    1]\n",
      " [1208    0    1]\n",
      " [1239    0    1]\n",
      " [1269    0    1]\n",
      " [1290    0    1]\n",
      " [1301    0    1]\n",
      " [1336    0    1]\n",
      " [1344    0    1]\n",
      " [1383    0    1]\n",
      " [1409    0    1]\n",
      " [1494    0    1]\n",
      " [1505    0    1]\n",
      " [1514    0    1]\n",
      " [1542    0    1]\n",
      " [1555    0    1]\n",
      " [1605    0    1]\n",
      " [1616    0    1]\n",
      " [1701    0    1]\n",
      " [1714    0    1]\n",
      " [1790    0    1]\n",
      " [1807    0    1]\n",
      " [1824    0    1]\n",
      " [1835    0    1]\n",
      " [1853    0    1]\n",
      " [1885    0    1]\n",
      " [1895    0    1]\n",
      " [1916    0    1]\n",
      " [1956    0    1]\n",
      " [1997    0    1]\n",
      " [2036    0    1]\n",
      " [2069    0    1]\n",
      " [2123    0    1]\n",
      " [2139    0    1]\n",
      " [2365    0    1]\n",
      " [2385    0    1]\n",
      " [2405    0    1]\n",
      " [2430    0    1]\n",
      " [2505    0    1]\n",
      " [2515    0    1]\n",
      " [2539    0    1]\n",
      " [2563    0    1]\n",
      " [2629    0    1]\n",
      " [2654    0    1]\n",
      " [2677    0    1]\n",
      " [2686    0    1]\n",
      " [2705    0    1]\n",
      " [2751    0    1]\n",
      " [2764    0    1]\n",
      " [2772    0    1]\n",
      " [2802    0    1]\n",
      " [2829    0    1]\n",
      " [2849    0    1]\n",
      " [2861    0    1]\n",
      " [2886    0    1]\n",
      " [2918    0    1]\n",
      " [2967    0    1]\n",
      " [2981    0    1]\n",
      " [3100    0    1]\n",
      " [3127    0    1]\n",
      " [3144    0    1]\n",
      " [3190    0    1]\n",
      " [3201    0    1]\n",
      " [3238    0    1]\n",
      " [3250    0    1]\n",
      " [3294    0    1]\n",
      " [3342    0    1]\n",
      " [3360    0    1]\n",
      " [3373    0    1]\n",
      " [3400    0    1]\n",
      " [3408    0    1]\n",
      " [3449    0    1]\n",
      " [3458    0    1]\n",
      " [3497    0    1]\n",
      " [3515    0    1]\n",
      " [3530    0    1]\n",
      " [3564    0    1]\n",
      " [3571    0    1]\n",
      " [3696    0    1]\n",
      " [3716    0    1]\n",
      " [3820    0    1]\n",
      " [3832    0    1]\n",
      " [3869    0    1]\n",
      " [3910    0    1]\n",
      " [3927    0    1]\n",
      " [3964    0    1]\n",
      " [4007    0    1]\n",
      " [4028    0    1]\n",
      " [4069    0    1]\n",
      " [4089    0    1]\n",
      " [4259    0    1]\n",
      " [4276    0    1]\n",
      " [4293    0    1]\n",
      " [4330    0    1]\n",
      " [4359    0    1]\n",
      " [4417    0    1]\n",
      " [4490    0    1]\n",
      " [4506    0    1]\n",
      " [4595    0    1]\n",
      " [4617    0    1]\n",
      " [4633    0    1]\n",
      " [4674    0    1]\n",
      " [4693    0    1]\n",
      " [4703    0    1]\n",
      " [4726    0    1]\n",
      " [4760    0    1]\n",
      " [4791    0    1]\n",
      " [4808    0    1]\n",
      " [4818    0    1]\n",
      " [4828    0    1]\n",
      " [4859    0    1]\n",
      " [4876    0    1]\n",
      " [4893    0    1]\n",
      " [4903    0    1]\n",
      " [4938    0    1]\n",
      " [4962    0    1]\n",
      " [4981    0    1]\n",
      " [5093    0    1]\n",
      " [5106    0    1]\n",
      " [5178    0    1]\n",
      " [5190    0    1]\n",
      " [5243    0    1]\n",
      " [5250    0    1]\n",
      " [5270    0    1]\n",
      " [5284    0    1]\n",
      " [5419    0    1]\n",
      " [5431    0    1]\n",
      " [5449    0    1]\n",
      " [5473    0    1]\n",
      " [5485    0    1]\n",
      " [5518    0    1]\n",
      " [5585    0    1]\n",
      " [5594    0    1]\n",
      " [5634    0    1]\n",
      " [5646    0    1]\n",
      " [5668    0    1]\n",
      " [5705    0    1]\n",
      " [5728    0    1]\n",
      " [5751    0    1]\n",
      " [5768    0    1]\n",
      " [5792    0    1]\n",
      " [5825    0    1]\n",
      " [5849    0    1]\n",
      " [5882    0    1]\n",
      " [5895    0    1]\n",
      " [5918    0    1]\n",
      " [5927    0    1]\n",
      " [5975    0    1]\n",
      " [5986    0    1]\n",
      " [6001    0    1]\n",
      " [6032    0    1]\n",
      " [6056    0    1]]\n",
      "<Epochs |  174 events (all good), -0.3 – 1.2 s, baseline off, ~11.9 MB, data loaded,\n",
      " '1': 174>\n",
      "subject_num= S13_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "tape_start_datapoints_npINT64= 853\n",
      "abs_wOnsets_dta_ndarray= [ 857.6     909.2721  931.4543  978.6929  988.2925 1014.6327 1084.0749\n",
      " 1132.9918 1146.8712 1183.449  1201.4082 1218.4596 1263.6667 1280.4286\n",
      " 1292.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [ 857  909  931  978  988 1014 1084 1132 1146 1183 1201 1218 1263 1280\n",
      " 1292]\n",
      "[[ 857    0    1]\n",
      " [ 909    0    1]\n",
      " [ 931    0    1]\n",
      " [ 978    0    1]\n",
      " [ 988    0    1]\n",
      " [1014    0    1]\n",
      " [1084    0    1]\n",
      " [1132    0    1]\n",
      " [1146    0    1]\n",
      " [1183    0    1]\n",
      " [1201    0    1]\n",
      " [1218    0    1]\n",
      " [1263    0    1]\n",
      " [1280    0    1]\n",
      " [1292    0    1]\n",
      " [1331    0    1]\n",
      " [1343    0    1]\n",
      " [1356    0    1]\n",
      " [1396    0    1]\n",
      " [1442    0    1]\n",
      " [1458    0    1]\n",
      " [1567    0    1]\n",
      " [1591    0    1]\n",
      " [1602    0    1]\n",
      " [1648    0    1]\n",
      " [1679    0    1]\n",
      " [1709    0    1]\n",
      " [1730    0    1]\n",
      " [1741    0    1]\n",
      " [1776    0    1]\n",
      " [1784    0    1]\n",
      " [1823    0    1]\n",
      " [1849    0    1]\n",
      " [1934    0    1]\n",
      " [1945    0    1]\n",
      " [1954    0    1]\n",
      " [1982    0    1]\n",
      " [1995    0    1]\n",
      " [2045    0    1]\n",
      " [2056    0    1]\n",
      " [2141    0    1]\n",
      " [2154    0    1]\n",
      " [2230    0    1]\n",
      " [2247    0    1]\n",
      " [2264    0    1]\n",
      " [2275    0    1]\n",
      " [2293    0    1]\n",
      " [2325    0    1]\n",
      " [2335    0    1]\n",
      " [2356    0    1]\n",
      " [2396    0    1]\n",
      " [2437    0    1]\n",
      " [2476    0    1]\n",
      " [2509    0    1]\n",
      " [2563    0    1]\n",
      " [2579    0    1]\n",
      " [2805    0    1]\n",
      " [2825    0    1]\n",
      " [2845    0    1]\n",
      " [2870    0    1]\n",
      " [2945    0    1]\n",
      " [2955    0    1]\n",
      " [2979    0    1]\n",
      " [3003    0    1]\n",
      " [3069    0    1]\n",
      " [3094    0    1]\n",
      " [3117    0    1]\n",
      " [3126    0    1]\n",
      " [3145    0    1]\n",
      " [3191    0    1]\n",
      " [3204    0    1]\n",
      " [3212    0    1]\n",
      " [3242    0    1]\n",
      " [3269    0    1]\n",
      " [3289    0    1]\n",
      " [3301    0    1]\n",
      " [3326    0    1]\n",
      " [3358    0    1]\n",
      " [3407    0    1]\n",
      " [3421    0    1]\n",
      " [3540    0    1]\n",
      " [3567    0    1]\n",
      " [3584    0    1]\n",
      " [3630    0    1]\n",
      " [3641    0    1]\n",
      " [3678    0    1]\n",
      " [3690    0    1]\n",
      " [3734    0    1]\n",
      " [3782    0    1]\n",
      " [3800    0    1]\n",
      " [3813    0    1]\n",
      " [3840    0    1]\n",
      " [3848    0    1]\n",
      " [3889    0    1]\n",
      " [3898    0    1]\n",
      " [3937    0    1]\n",
      " [3955    0    1]\n",
      " [3970    0    1]\n",
      " [4004    0    1]\n",
      " [4011    0    1]\n",
      " [4136    0    1]\n",
      " [4156    0    1]\n",
      " [4260    0    1]\n",
      " [4272    0    1]\n",
      " [4309    0    1]\n",
      " [4350    0    1]\n",
      " [4367    0    1]\n",
      " [4404    0    1]\n",
      " [4447    0    1]\n",
      " [4468    0    1]\n",
      " [4509    0    1]\n",
      " [4529    0    1]\n",
      " [4699    0    1]\n",
      " [4716    0    1]\n",
      " [4733    0    1]\n",
      " [4770    0    1]\n",
      " [4799    0    1]\n",
      " [4857    0    1]\n",
      " [4930    0    1]\n",
      " [4946    0    1]\n",
      " [5035    0    1]\n",
      " [5057    0    1]\n",
      " [5073    0    1]\n",
      " [5114    0    1]\n",
      " [5133    0    1]\n",
      " [5143    0    1]\n",
      " [5166    0    1]\n",
      " [5200    0    1]\n",
      " [5231    0    1]\n",
      " [5248    0    1]\n",
      " [5258    0    1]\n",
      " [5268    0    1]\n",
      " [5299    0    1]\n",
      " [5316    0    1]\n",
      " [5333    0    1]\n",
      " [5343    0    1]\n",
      " [5378    0    1]\n",
      " [5402    0    1]\n",
      " [5421    0    1]\n",
      " [5533    0    1]\n",
      " [5546    0    1]\n",
      " [5618    0    1]\n",
      " [5630    0    1]\n",
      " [5683    0    1]\n",
      " [5690    0    1]\n",
      " [5710    0    1]\n",
      " [5724    0    1]\n",
      " [5859    0    1]\n",
      " [5871    0    1]\n",
      " [5889    0    1]\n",
      " [5913    0    1]\n",
      " [5925    0    1]\n",
      " [5958    0    1]\n",
      " [6025    0    1]\n",
      " [6034    0    1]\n",
      " [6074    0    1]\n",
      " [6086    0    1]\n",
      " [6108    0    1]\n",
      " [6145    0    1]\n",
      " [6168    0    1]\n",
      " [6191    0    1]\n",
      " [6208    0    1]\n",
      " [6232    0    1]\n",
      " [6265    0    1]\n",
      " [6289    0    1]\n",
      " [6322    0    1]\n",
      " [6335    0    1]\n",
      " [6358    0    1]\n",
      " [6367    0    1]\n",
      " [6415    0    1]\n",
      " [6426    0    1]\n",
      " [6441    0    1]\n",
      " [6472    0    1]\n",
      " [6496    0    1]]\n",
      "<Epochs |  174 events (all good), -0.3 – 1.2 s, baseline off, ~11.9 MB, data loaded,\n",
      " '1': 174>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tape_word_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m     tape_perTape_epochs \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mEpochs(raw, events\u001b[38;5;241m=\u001b[39mwOnset_perTape_events, event_id\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, tmin\u001b[38;5;241m=\u001b[39mtmin, tmax\u001b[38;5;241m=\u001b[39mtmax, baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, preload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tape_perTape_epochs)\n\u001b[0;32m---> 56\u001b[0m all_tapes_epochs\u001b[38;5;241m.\u001b[39mappend(tape_word_epochs)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mfor i, stimulus_idx in enumerate(trial_indexes):\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    # We find the start of the tape in the raw EEG\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m# eeg_data = ds_all_words['eeg'].get_data()\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tape_word_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "## Trying the old way (Tape order 1st then subject cut 2nd)##\n",
    "\n",
    "# epochs parameters\n",
    "# Define your epoch window\n",
    "# We use a 1.5s window (including buffers for HHSA)\n",
    "tmin, tmax = -0.3, 1.2 \n",
    "\n",
    "# Get the actual word onset based on EEG triggers datapoints\n",
    "for i, stimulus_idx in enumerate(trial_indexes):\n",
    "    # Find the word onset time based on the segment sequence    \n",
    "    wOnset_perTape_DF = word_onset_essentials_DF.loc[word_onset_essentials_DF[\"Segment\"] == i+1, :] #.to_numpy()\n",
    "    #wOnset_time_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy() #*raw_sfreq\n",
    "    wOnset_datapoints_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy()*raw_sfreq\n",
    "    print(\"wOnset_datapoints_ndarray=\", wOnset_datapoints_ndarray[0:15], type(wOnset_datapoints_ndarray))\n",
    "\n",
    "    for subject in SUBJECTS[0:3]:\n",
    "        print(\"subject_num=\", subject)\n",
    "        print(\"tape_num=\", i+1)\n",
    "        \n",
    "        # 1. Load Raw as an Eelbrain-compatible object\n",
    "        raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)\n",
    "        raw_sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # 2. Get the events for the 12 tapes\n",
    "        events_DICT = eelbrain.load.mne.events(raw)\n",
    "\n",
    "        # Get the tape start time\n",
    "        tape_start_datapoints_npINT64 = events_DICT[i]['i_start']\n",
    "        #tape_start_time_npFLOAT64 = tape_start_datapoints_npINT64 / raw_sfreq\n",
    "        print(\"tape_start_datapoints_npINT64=\", tape_start_datapoints_npINT64)\n",
    "        #print(\"tape_start_time_npFLOAT64=\", tape_start_time_npFLOAT64, type(tape_start_time_npFLOAT64))\n",
    "\n",
    "        # Get the actual word onset time by the triggers\n",
    "        #absolute_onsets_time_ndarray = tape_start_time_npFLOAT64 + wOnset_time_ndarray\n",
    "        #print(\"absolute_onsets_time_ndarray=\", absolute_onsets_time_ndarray[0:15])\n",
    "        abs_wOnsets_dta_ndarray = tape_start_datapoints_npINT64 + wOnset_datapoints_ndarray\n",
    "        print(\"abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "        \n",
    "        # To exclude the decimal but leave the integer along, and turn FLOAT into INT\n",
    "        abs_wOnsets_dta_ndarray = np.trunc(abs_wOnsets_dta_ndarray).astype(int)\n",
    "        print(\"rounded_abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "\n",
    "        # Make epochs\n",
    "        # Create the empty (N, 3) event matrix based on wOnset per tape\n",
    "        wOnset_events = len(abs_wOnsets_dta_ndarray)\n",
    "        wOnset_perTape_events = np.zeros((wOnset_events, 3), dtype=int)\n",
    "        # Fill the columns\n",
    "        wOnset_perTape_events[:, 0] = abs_wOnsets_dta_ndarray  # Column 0: The sample indices\n",
    "        wOnset_perTape_events[:, 2] = i+1           # Column 2: The event ID (e.g., 1)\n",
    "        print(wOnset_perTape_events)\n",
    "\n",
    "        \n",
    "        #word_perTape_epochs = mne.epochs(raw, tmin=tmin, tmax=tmax, baseline=None, events=wOnset_perTape_events)\n",
    "        tape_perTape_epochs = mne.Epochs(raw, events=wOnset_perTape_events, event_id=i+1, tmin=tmin, tmax=tmax, baseline=None, preload=True)\n",
    "        print(tape_perTape_epochs)\n",
    "    all_tapes_epochs.append(tape_word_epochs)\n",
    "        \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    for i, stimulus_idx in enumerate(trial_indexes):\n",
    "        # We find the start of the tape in the raw EEG\n",
    "        tape_start = events[i]['i_start']#['time']\n",
    "        print([i], tape_start, type(tape_start))\n",
    "        \n",
    "        # Get word onset times for this specific tape from your list\n",
    "        # word_onsets[stimulus_idx] is your impulse predictor\n",
    "        onsets = word_onsets[stimulus_idx].time.times[word_onsets[stimulus_idx].x > 0]\n",
    "        print(onsets, len(onsets))\n",
    "    \n",
    "        \n",
    "        # Convert relative word times to absolute EEG times\n",
    "        absolute_onsets = tape_start + onsets\n",
    "        \n",
    "        # Create segments (Epochs) directly in Eelbrain\n",
    "        # This is much faster and more accurate than manual padding/cropping\n",
    "        tape_word_epochs = mne.epochs(\n",
    "            raw, tmin=tmin, tmax=tmax, baseline=None, \n",
    "            events=absolute_onsets\n",
    "        )\n",
    "        all_tapes_epochs.append(tape_word_epochs)\n",
    "    \n",
    "    # 5. Combine all words from all tapes into one Dataset\n",
    "    # This 'ds' will have a column for 'EEG' and can have a column for 'Subject'\n",
    "    ds_all_words = eelbrain.combine(all_tapes_epochs)\n",
    "    \n",
    "    # Now you can access the data for HHSA:\n",
    "    # eeg_data = ds_all_words['eeg'].get_data()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bc4553ed-852e-47c9-be5b-c28dd2079f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_num= S44_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "0        Alice        1   0.046000\n",
      "1          was        1   0.562721\n",
      "2    beginning        1   0.784543\n",
      "3           to        1   1.256929\n",
      "4          get        1   1.352925\n",
      "..         ...      ...        ...\n",
      "169         it        1  55.626219\n",
      "170        all        1  55.734441\n",
      "171     seemed        1  55.885951\n",
      "172      quite        1  56.195464\n",
      "173    natural        1  56.439728\n",
      "\n",
      "[174 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [  4.6     56.2721  78.4543 125.6929 135.2925 161.6327 231.0749 279.9918\n",
      " 293.8712 330.449  348.4082 365.4596 410.6667 427.4286 439.8799] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 172\n",
      "abs_wOnsets_dta_ndarray= [176.6    228.2721 250.4543 297.6929 307.2925 333.6327 403.0749 451.9918\n",
      " 465.8712 502.449  520.4082 537.4596 582.6667 599.4286 611.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [176 228 250 297 307 333 403 451 465 502 520 537 582 599 611]\n",
      "tape_num= 2\n",
      "wOnset_perTape_DF=          Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 47.9839572  59.2424402  81.0806402  91.2952402 141.5810402 191.8667402\n",
      " 238.5605402 255.7777402 321.1728402 343.1803402 359.5218402 379.0771402\n",
      " 438.4907402 563.7331402 574.7717402] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 5934\n",
      "abs_wOnsets_dta_ndarray= [5981.9839572 5993.2424402 6015.0806402 6025.2952402 6075.5810402\n",
      " 6125.8667402 6172.5605402 6189.7777402 6255.1728402 6277.1803402\n",
      " 6293.5218402 6313.0771402 6372.4907402 6497.7331402 6508.7717402]\n",
      "rounded_abs_wOnsets_dta_ndarray= [5981 5993 6015 6025 6075 6125 6172 6189 6255 6277 6293 6313 6372 6497\n",
      " 6508]\n",
      "tape_num= 3\n",
      "wOnset_perTape_DF=       Word  Segment      onset\n",
      "351  First        3   0.478723\n",
      "352    she        3   0.986270\n",
      "353  tried        3   1.201781\n",
      "354     to        3   1.501100\n",
      "355   look        3   1.604643\n",
      "..     ...      ...        ...\n",
      "530   this        3  61.269264\n",
      "531   time        3  61.591394\n",
      "532    she        3  62.071440\n",
      "533   said        3  62.263005\n",
      "534  aloud        3  62.538379\n",
      "\n",
      "[184 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 47.8723402  98.6269787 120.1780787 150.1099787 160.4642787 183.6337787\n",
      " 218.3548787 231.5249787 260.2596787 284.2052787 296.1780787 314.3827787\n",
      " 333.2936787 371.6065787 450.6269787] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 12023\n",
      "abs_wOnsets_dta_ndarray= [12070.8723402 12121.6269787 12143.1780787 12173.1099787 12183.4642787\n",
      " 12206.6337787 12241.3548787 12254.5249787 12283.2596787 12307.2052787\n",
      " 12319.1780787 12337.3827787 12356.2936787 12394.6065787 12473.6269787]\n",
      "rounded_abs_wOnsets_dta_ndarray= [12070 12121 12143 12173 12183 12206 12241 12254 12283 12307 12319 12337\n",
      " 12356 12394 12473]\n",
      "tape_num= 4\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "535       must        4   0.398386\n",
      "536         be        4   0.689428\n",
      "537    getting        4   0.840938\n",
      "538  somewhere        4   1.126350\n",
      "539       near        4   1.542892\n",
      "..         ...      ...        ...\n",
      "744      think        4  68.262788\n",
      "745        you        4  68.437370\n",
      "746      could        4  68.593016\n",
      "747     manage        4  68.760635\n",
      "748         it        4  69.129919\n",
      "\n",
      "[214 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 39.8385787  68.9427631  84.0937631 112.6349631 154.2891631 177.2879631\n",
      " 184.4716631 225.1791631 234.7573631 248.5197631 312.5805631 335.3288631\n",
      " 347.3015631 380.8254631 399.0834631] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 18355\n",
      "abs_wOnsets_dta_ndarray= [18394.8385787 18423.9427631 18439.0937631 18467.6349631 18509.2891631\n",
      " 18532.2879631 18539.4716631 18580.1791631 18589.7573631 18603.5197631\n",
      " 18667.5805631 18690.3288631 18702.3015631 18735.8254631 18754.0834631]\n",
      "rounded_abs_wOnsets_dta_ndarray= [18394 18423 18439 18467 18509 18532 18539 18580 18589 18603 18667 18690\n",
      " 18702 18735 18754]\n",
      "tape_num= 5\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "749        And        5   0.561146\n",
      "750       what        5   0.689847\n",
      "751         an        5   0.936113\n",
      "752   ignorant        5   1.115705\n",
      "753     little        5   1.618562\n",
      "..         ...      ...        ...\n",
      "937     saying        5  64.085883\n",
      "938         to        5  64.547542\n",
      "939        her        5  64.691215\n",
      "940       very        5  64.858834\n",
      "941  earnestly        5  65.313800\n",
      "\n",
      "[193 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 56.1145631  68.9847173  93.6113173 111.5705173 161.8562173 185.8018173\n",
      " 210.7547173 222.0976173 231.2984173 252.8494173 264.8222173 284.7481173\n",
      " 410.8902173 452.7950173 466.6011173] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 25359\n",
      "abs_wOnsets_dta_ndarray= [25415.1145631 25427.9847173 25452.6113173 25470.5705173 25520.8562173\n",
      " 25544.8018173 25569.7547173 25581.0976173 25590.2984173 25611.8494173\n",
      " 25623.8222173 25643.7481173 25769.8902173 25811.7950173 25825.6011173]\n",
      "rounded_abs_wOnsets_dta_ndarray= [25415 25427 25452 25470 25520 25544 25569 25581 25590 25611 25623 25643\n",
      " 25769 25811 25825]\n",
      "tape_num= 6\n",
      "wOnset_perTape_DF=        Word  Segment      onset\n",
      "942     Now        6   0.323245\n",
      "943   Dinah        6   0.568709\n",
      "944    tell        6   1.039809\n",
      "945      me        6   1.422938\n",
      "946     the        6   1.590557\n",
      "...     ...      ...        ...\n",
      "1134   ever        6  61.825659\n",
      "1135     to        6  62.101034\n",
      "1136    get        6  62.268653\n",
      "1137    out        6  62.436272\n",
      "1138  again        6  62.663755\n",
      "\n",
      "[197 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 32.3245298  56.8708743 103.9808743 142.2937743 159.0556743 173.0401743\n",
      " 238.0761743 260.0501743 284.7699743 320.6883743 337.9041743 353.4880743\n",
      " 473.9400743 496.7131743 597.2597743] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 31990\n",
      "abs_wOnsets_dta_ndarray= [32022.3245298 32046.8708743 32093.9808743 32132.2937743 32149.0556743\n",
      " 32163.0401743 32228.0761743 32250.0501743 32274.7699743 32310.6883743\n",
      " 32327.9041743 32343.4880743 32463.9400743 32486.7131743 32587.2597743]\n",
      "rounded_abs_wOnsets_dta_ndarray= [32022 32046 32093 32132 32149 32163 32228 32250 32274 32310 32327 32343\n",
      " 32463 32486 32587]\n",
      "tape_num= 7\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1139  Suddenly        7   0.586736\n",
      "1140       she        7   1.179532\n",
      "1141      came        7   1.418988\n",
      "1142      upon        7   1.694362\n",
      "1143         a        7   2.051148\n",
      "...        ...      ...        ...\n",
      "1312       her        7  61.175179\n",
      "1313      head        7  61.294906\n",
      "1314    though        7  61.675837\n",
      "1315       the        7  61.845655\n",
      "1316   doorway        7  61.965383\n",
      "\n",
      "[178 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 58.6736359 117.9532195 141.8988195 169.4362195 205.1148195 230.4974195\n",
      " 274.7968195 299.9396195 337.0553195 425.6539195 446.0076195 471.1505195\n",
      " 479.5315195 527.4226195 650.7423195] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 38375\n",
      "abs_wOnsets_dta_ndarray= [38433.6736359 38492.9532195 38516.8988195 38544.4362195 38580.1148195\n",
      " 38605.4974195 38649.7968195 38674.9396195 38712.0553195 38800.6539195\n",
      " 38821.0076195 38846.1505195 38854.5315195 38902.4226195 39025.7423195]\n",
      "rounded_abs_wOnsets_dta_ndarray= [38433 38492 38516 38544 38580 38605 38649 38674 38712 38800 38821 38846\n",
      " 38854 38902 39025]\n",
      "tape_num= 8\n",
      "wOnset_perTape_DF=        Word  Segment      onset\n",
      "1317    and        8   0.342000\n",
      "1318   even        8   0.453317\n",
      "1319     if        8   0.696699\n",
      "1320     my        8   0.960100\n",
      "1321   head        8   1.067855\n",
      "...     ...      ...        ...\n",
      "1487     do        8  55.663774\n",
      "1488   that        8  55.855338\n",
      "1489     in        8  56.190576\n",
      "1490      a        8  56.366154\n",
      "1491  hurry        8  56.444074\n",
      "\n",
      "[175 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 34.2        45.3316684  69.6698684  96.0099684 106.7854684 139.1120684\n",
      " 158.2684684 178.5899684 250.4589684 277.9963684 304.3431684 420.4725684\n",
      " 428.9338684 448.0099684 467.1664684] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 44671\n",
      "abs_wOnsets_dta_ndarray= [44705.2       44716.3316684 44740.6698684 44767.0099684 44777.7854684\n",
      " 44810.1120684 44829.2684684 44849.5899684 44921.4589684 44948.9963684\n",
      " 44975.3431684 45091.4725684 45099.9338684 45119.0099684 45138.1664684]\n",
      "rounded_abs_wOnsets_dta_ndarray= [44705 44716 44740 44767 44777 44810 44829 44849 44921 44948 44975 45091\n",
      " 45099 45119 45138]\n",
      "tape_num= 9\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1492        No        9   0.432017\n",
      "1493         I        9   1.046815\n",
      "1494        ll        9   1.198494\n",
      "1495      look        9   1.394026\n",
      "1496     first        9   1.753210\n",
      "...        ...      ...        ...\n",
      "1643      very        9  54.086271\n",
      "1644      soon        9  54.397564\n",
      "1645  finished        9  55.044094\n",
      "1646        it        9  55.423901\n",
      "1647       off        9  55.536451\n",
      "\n",
      "[156 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 43.2017422 104.6814784 119.8493784 139.4025784 175.3209784 213.3840784\n",
      " 235.1848784 270.9578784 278.9662784 311.8107784 344.1372784 353.1518784\n",
      " 362.0964784 398.0148784 447.0263784] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 50409\n",
      "abs_wOnsets_dta_ndarray= [50452.2017422 50513.6814784 50528.8493784 50548.4025784 50584.3209784\n",
      " 50622.3840784 50644.1848784 50679.9578784 50687.9662784 50720.8107784\n",
      " 50753.1372784 50762.1518784 50771.0964784 50807.0148784 50856.0263784]\n",
      "rounded_abs_wOnsets_dta_ndarray= [50452 50513 50528 50548 50584 50622 50644 50679 50687 50720 50753 50762\n",
      " 50771 50807 50856]\n",
      "tape_num= 10\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1648      What       10   1.051981\n",
      "1649         a       10   1.160767\n",
      "1650   curious       10   1.308375\n",
      "1651   feeling       10   2.146470\n",
      "1652      said       10   2.787119\n",
      "...        ...      ...        ...\n",
      "1830     could       10  59.244701\n",
      "1831       not       10  59.448239\n",
      "1832  possibly       10  59.855313\n",
      "1833     reach       10  60.430007\n",
      "1834        it       10  60.784961\n",
      "\n",
      "[187 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [105.1980798 116.0767    130.8375    214.647     278.7119    306.9088\n",
      " 367.0205    381.0893    411.0007    419.3817    460.2269    478.0483\n",
      " 490.7454    500.4853    645.6674   ] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 56136\n",
      "abs_wOnsets_dta_ndarray= [56241.1980798 56252.0767    56266.8375    56350.647     56414.7119\n",
      " 56442.9088    56503.0205    56517.0893    56547.0007    56555.3817\n",
      " 56596.2269    56614.0483    56626.7454    56636.4853    56781.6674   ]\n",
      "rounded_abs_wOnsets_dta_ndarray= [56241 56252 56266 56350 56414 56442 56503 56517 56547 56555 56596 56614\n",
      " 56626 56636 56781]\n",
      "tape_num= 11\n",
      "wOnset_perTape_DF=              Word  Segment      onset\n",
      "1835          she       11   0.274485\n",
      "1836        could       11   0.489783\n",
      "1837          see       11   0.701863\n",
      "1838           it       11   1.064477\n",
      "1839        quite       11   1.279987\n",
      "...           ...      ...        ...\n",
      "1987           to       11  53.797174\n",
      "1988         make       11  53.890245\n",
      "1989          one       11  54.211688\n",
      "1990  respectable       11  54.517325\n",
      "1991       person       11  55.331149\n",
      "\n",
      "[157 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 27.448538   48.9783265  70.1863265 106.4477265 127.9987265 161.5225265\n",
      " 213.0055265 236.9511265 245.3320265 357.8762265 369.5823265 386.6109265\n",
      " 420.1347265 433.3048265 489.5769265] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 62268\n",
      "abs_wOnsets_dta_ndarray= [62295.448538  62316.9783265 62338.1863265 62374.4477265 62395.9987265\n",
      " 62429.5225265 62481.0055265 62504.9511265 62513.3320265 62625.8762265\n",
      " 62637.5823265 62654.6109265 62688.1347265 62701.3048265 62757.5769265]\n",
      "rounded_abs_wOnsets_dta_ndarray= [62295 62316 62338 62374 62395 62429 62481 62504 62513 62625 62637 62654\n",
      " 62688 62701 62757]\n",
      "tape_num= 12\n",
      "wOnset_perTape_DF=          Word  Segment      onset\n",
      "1992     Soon       12   0.229476\n",
      "1993      her       12   0.504358\n",
      "1994      eye       12   0.658032\n",
      "1995     fell       12   0.974924\n",
      "1996       on       12   1.266236\n",
      "...       ...      ...        ...\n",
      "2124  happens       12  45.226353\n",
      "2125     when       12  45.677924\n",
      "2126      one       12  45.896829\n",
      "2127     eats       12  46.064448\n",
      "2128     cake       12  46.327849\n",
      "\n",
      "[137 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 22.9476225  50.4358225  65.8032225  97.4924225 126.6236225 144.1863225\n",
      " 151.5146225 180.1046225 222.0094225 268.7033225 279.4788225 292.6489225\n",
      " 329.3501225 356.1046225 364.4856225] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 67891\n",
      "abs_wOnsets_dta_ndarray= [67913.9476225 67941.4358225 67956.8032225 67988.4924225 68017.6236225\n",
      " 68035.1863225 68042.5146225 68071.1046225 68113.0094225 68159.7033225\n",
      " 68170.4788225 68183.6489225 68220.3501225 68247.1046225 68255.4856225]\n",
      "rounded_abs_wOnsets_dta_ndarray= [67913 67941 67956 67988 68017 68035 68042 68071 68113 68159 68170 68183\n",
      " 68220 68247 68255]\n",
      "subject_num= S20_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "0        Alice        1   0.046000\n",
      "1          was        1   0.562721\n",
      "2    beginning        1   0.784543\n",
      "3           to        1   1.256929\n",
      "4          get        1   1.352925\n",
      "..         ...      ...        ...\n",
      "169         it        1  55.626219\n",
      "170        all        1  55.734441\n",
      "171     seemed        1  55.885951\n",
      "172      quite        1  56.195464\n",
      "173    natural        1  56.439728\n",
      "\n",
      "[174 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [  4.6     56.2721  78.4543 125.6929 135.2925 161.6327 231.0749 279.9918\n",
      " 293.8712 330.449  348.4082 365.4596 410.6667 427.4286 439.8799] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 413\n",
      "abs_wOnsets_dta_ndarray= [417.6    469.2721 491.4543 538.6929 548.2925 574.6327 644.0749 692.9918\n",
      " 706.8712 743.449  761.4082 778.4596 823.6667 840.4286 852.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [417 469 491 538 548 574 644 692 706 743 761 778 823 840 852]\n",
      "tape_num= 2\n",
      "wOnset_perTape_DF=          Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 47.9839572  59.2424402  81.0806402  91.2952402 141.5810402 191.8667402\n",
      " 238.5605402 255.7777402 321.1728402 343.1803402 359.5218402 379.0771402\n",
      " 438.4907402 563.7331402 574.7717402] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 6175\n",
      "abs_wOnsets_dta_ndarray= [6222.9839572 6234.2424402 6256.0806402 6266.2952402 6316.5810402\n",
      " 6366.8667402 6413.5605402 6430.7777402 6496.1728402 6518.1803402\n",
      " 6534.5218402 6554.0771402 6613.4907402 6738.7331402 6749.7717402]\n",
      "rounded_abs_wOnsets_dta_ndarray= [6222 6234 6256 6266 6316 6366 6413 6430 6496 6518 6534 6554 6613 6738\n",
      " 6749]\n",
      "tape_num= 3\n",
      "wOnset_perTape_DF=       Word  Segment      onset\n",
      "351  First        3   0.478723\n",
      "352    she        3   0.986270\n",
      "353  tried        3   1.201781\n",
      "354     to        3   1.501100\n",
      "355   look        3   1.604643\n",
      "..     ...      ...        ...\n",
      "530   this        3  61.269264\n",
      "531   time        3  61.591394\n",
      "532    she        3  62.071440\n",
      "533   said        3  62.263005\n",
      "534  aloud        3  62.538379\n",
      "\n",
      "[184 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 47.8723402  98.6269787 120.1780787 150.1099787 160.4642787 183.6337787\n",
      " 218.3548787 231.5249787 260.2596787 284.2052787 296.1780787 314.3827787\n",
      " 333.2936787 371.6065787 450.6269787] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 12270\n",
      "abs_wOnsets_dta_ndarray= [12317.8723402 12368.6269787 12390.1780787 12420.1099787 12430.4642787\n",
      " 12453.6337787 12488.3548787 12501.5249787 12530.2596787 12554.2052787\n",
      " 12566.1780787 12584.3827787 12603.2936787 12641.6065787 12720.6269787]\n",
      "rounded_abs_wOnsets_dta_ndarray= [12317 12368 12390 12420 12430 12453 12488 12501 12530 12554 12566 12584\n",
      " 12603 12641 12720]\n",
      "tape_num= 4\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "535       must        4   0.398386\n",
      "536         be        4   0.689428\n",
      "537    getting        4   0.840938\n",
      "538  somewhere        4   1.126350\n",
      "539       near        4   1.542892\n",
      "..         ...      ...        ...\n",
      "744      think        4  68.262788\n",
      "745        you        4  68.437370\n",
      "746      could        4  68.593016\n",
      "747     manage        4  68.760635\n",
      "748         it        4  69.129919\n",
      "\n",
      "[214 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 39.8385787  68.9427631  84.0937631 112.6349631 154.2891631 177.2879631\n",
      " 184.4716631 225.1791631 234.7573631 248.5197631 312.5805631 335.3288631\n",
      " 347.3015631 380.8254631 399.0834631] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 18601\n",
      "abs_wOnsets_dta_ndarray= [18640.8385787 18669.9427631 18685.0937631 18713.6349631 18755.2891631\n",
      " 18778.2879631 18785.4716631 18826.1791631 18835.7573631 18849.5197631\n",
      " 18913.5805631 18936.3288631 18948.3015631 18981.8254631 19000.0834631]\n",
      "rounded_abs_wOnsets_dta_ndarray= [18640 18669 18685 18713 18755 18778 18785 18826 18835 18849 18913 18936\n",
      " 18948 18981 19000]\n",
      "tape_num= 5\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "749        And        5   0.561146\n",
      "750       what        5   0.689847\n",
      "751         an        5   0.936113\n",
      "752   ignorant        5   1.115705\n",
      "753     little        5   1.618562\n",
      "..         ...      ...        ...\n",
      "937     saying        5  64.085883\n",
      "938         to        5  64.547542\n",
      "939        her        5  64.691215\n",
      "940       very        5  64.858834\n",
      "941  earnestly        5  65.313800\n",
      "\n",
      "[193 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 56.1145631  68.9847173  93.6113173 111.5705173 161.8562173 185.8018173\n",
      " 210.7547173 222.0976173 231.2984173 252.8494173 264.8222173 284.7481173\n",
      " 410.8902173 452.7950173 466.6011173] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 25605\n",
      "abs_wOnsets_dta_ndarray= [25661.1145631 25673.9847173 25698.6113173 25716.5705173 25766.8562173\n",
      " 25790.8018173 25815.7547173 25827.0976173 25836.2984173 25857.8494173\n",
      " 25869.8222173 25889.7481173 26015.8902173 26057.7950173 26071.6011173]\n",
      "rounded_abs_wOnsets_dta_ndarray= [25661 25673 25698 25716 25766 25790 25815 25827 25836 25857 25869 25889\n",
      " 26015 26057 26071]\n",
      "tape_num= 6\n",
      "wOnset_perTape_DF=        Word  Segment      onset\n",
      "942     Now        6   0.323245\n",
      "943   Dinah        6   0.568709\n",
      "944    tell        6   1.039809\n",
      "945      me        6   1.422938\n",
      "946     the        6   1.590557\n",
      "...     ...      ...        ...\n",
      "1134   ever        6  61.825659\n",
      "1135     to        6  62.101034\n",
      "1136    get        6  62.268653\n",
      "1137    out        6  62.436272\n",
      "1138  again        6  62.663755\n",
      "\n",
      "[197 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 32.3245298  56.8708743 103.9808743 142.2937743 159.0556743 173.0401743\n",
      " 238.0761743 260.0501743 284.7699743 320.6883743 337.9041743 353.4880743\n",
      " 473.9400743 496.7131743 597.2597743] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 32238\n",
      "abs_wOnsets_dta_ndarray= [32270.3245298 32294.8708743 32341.9808743 32380.2937743 32397.0556743\n",
      " 32411.0401743 32476.0761743 32498.0501743 32522.7699743 32558.6883743\n",
      " 32575.9041743 32591.4880743 32711.9400743 32734.7131743 32835.2597743]\n",
      "rounded_abs_wOnsets_dta_ndarray= [32270 32294 32341 32380 32397 32411 32476 32498 32522 32558 32575 32591\n",
      " 32711 32734 32835]\n",
      "tape_num= 7\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1139  Suddenly        7   0.586736\n",
      "1140       she        7   1.179532\n",
      "1141      came        7   1.418988\n",
      "1142      upon        7   1.694362\n",
      "1143         a        7   2.051148\n",
      "...        ...      ...        ...\n",
      "1312       her        7  61.175179\n",
      "1313      head        7  61.294906\n",
      "1314    though        7  61.675837\n",
      "1315       the        7  61.845655\n",
      "1316   doorway        7  61.965383\n",
      "\n",
      "[178 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 58.6736359 117.9532195 141.8988195 169.4362195 205.1148195 230.4974195\n",
      " 274.7968195 299.9396195 337.0553195 425.6539195 446.0076195 471.1505195\n",
      " 479.5315195 527.4226195 650.7423195] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 38623\n",
      "abs_wOnsets_dta_ndarray= [38681.6736359 38740.9532195 38764.8988195 38792.4362195 38828.1148195\n",
      " 38853.4974195 38897.7968195 38922.9396195 38960.0553195 39048.6539195\n",
      " 39069.0076195 39094.1505195 39102.5315195 39150.4226195 39273.7423195]\n",
      "rounded_abs_wOnsets_dta_ndarray= [38681 38740 38764 38792 38828 38853 38897 38922 38960 39048 39069 39094\n",
      " 39102 39150 39273]\n",
      "tape_num= 8\n",
      "wOnset_perTape_DF=        Word  Segment      onset\n",
      "1317    and        8   0.342000\n",
      "1318   even        8   0.453317\n",
      "1319     if        8   0.696699\n",
      "1320     my        8   0.960100\n",
      "1321   head        8   1.067855\n",
      "...     ...      ...        ...\n",
      "1487     do        8  55.663774\n",
      "1488   that        8  55.855338\n",
      "1489     in        8  56.190576\n",
      "1490      a        8  56.366154\n",
      "1491  hurry        8  56.444074\n",
      "\n",
      "[175 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 34.2        45.3316684  69.6698684  96.0099684 106.7854684 139.1120684\n",
      " 158.2684684 178.5899684 250.4589684 277.9963684 304.3431684 420.4725684\n",
      " 428.9338684 448.0099684 467.1664684] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 44917\n",
      "abs_wOnsets_dta_ndarray= [44951.2       44962.3316684 44986.6698684 45013.0099684 45023.7854684\n",
      " 45056.1120684 45075.2684684 45095.5899684 45167.4589684 45194.9963684\n",
      " 45221.3431684 45337.4725684 45345.9338684 45365.0099684 45384.1664684]\n",
      "rounded_abs_wOnsets_dta_ndarray= [44951 44962 44986 45013 45023 45056 45075 45095 45167 45194 45221 45337\n",
      " 45345 45365 45384]\n",
      "tape_num= 9\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1492        No        9   0.432017\n",
      "1493         I        9   1.046815\n",
      "1494        ll        9   1.198494\n",
      "1495      look        9   1.394026\n",
      "1496     first        9   1.753210\n",
      "...        ...      ...        ...\n",
      "1643      very        9  54.086271\n",
      "1644      soon        9  54.397564\n",
      "1645  finished        9  55.044094\n",
      "1646        it        9  55.423901\n",
      "1647       off        9  55.536451\n",
      "\n",
      "[156 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 43.2017422 104.6814784 119.8493784 139.4025784 175.3209784 213.3840784\n",
      " 235.1848784 270.9578784 278.9662784 311.8107784 344.1372784 353.1518784\n",
      " 362.0964784 398.0148784 447.0263784] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 50655\n",
      "abs_wOnsets_dta_ndarray= [50698.2017422 50759.6814784 50774.8493784 50794.4025784 50830.3209784\n",
      " 50868.3840784 50890.1848784 50925.9578784 50933.9662784 50966.8107784\n",
      " 50999.1372784 51008.1518784 51017.0964784 51053.0148784 51102.0263784]\n",
      "rounded_abs_wOnsets_dta_ndarray= [50698 50759 50774 50794 50830 50868 50890 50925 50933 50966 50999 51008\n",
      " 51017 51053 51102]\n",
      "tape_num= 10\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1648      What       10   1.051981\n",
      "1649         a       10   1.160767\n",
      "1650   curious       10   1.308375\n",
      "1651   feeling       10   2.146470\n",
      "1652      said       10   2.787119\n",
      "...        ...      ...        ...\n",
      "1830     could       10  59.244701\n",
      "1831       not       10  59.448239\n",
      "1832  possibly       10  59.855313\n",
      "1833     reach       10  60.430007\n",
      "1834        it       10  60.784961\n",
      "\n",
      "[187 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [105.1980798 116.0767    130.8375    214.647     278.7119    306.9088\n",
      " 367.0205    381.0893    411.0007    419.3817    460.2269    478.0483\n",
      " 490.7454    500.4853    645.6674   ] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 56383\n",
      "abs_wOnsets_dta_ndarray= [56488.1980798 56499.0767    56513.8375    56597.647     56661.7119\n",
      " 56689.9088    56750.0205    56764.0893    56794.0007    56802.3817\n",
      " 56843.2269    56861.0483    56873.7454    56883.4853    57028.6674   ]\n",
      "rounded_abs_wOnsets_dta_ndarray= [56488 56499 56513 56597 56661 56689 56750 56764 56794 56802 56843 56861\n",
      " 56873 56883 57028]\n",
      "tape_num= 11\n",
      "wOnset_perTape_DF=              Word  Segment      onset\n",
      "1835          she       11   0.274485\n",
      "1836        could       11   0.489783\n",
      "1837          see       11   0.701863\n",
      "1838           it       11   1.064477\n",
      "1839        quite       11   1.279987\n",
      "...           ...      ...        ...\n",
      "1987           to       11  53.797174\n",
      "1988         make       11  53.890245\n",
      "1989          one       11  54.211688\n",
      "1990  respectable       11  54.517325\n",
      "1991       person       11  55.331149\n",
      "\n",
      "[157 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 27.448538   48.9783265  70.1863265 106.4477265 127.9987265 161.5225265\n",
      " 213.0055265 236.9511265 245.3320265 357.8762265 369.5823265 386.6109265\n",
      " 420.1347265 433.3048265 489.5769265] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 62514\n",
      "abs_wOnsets_dta_ndarray= [62541.448538  62562.9783265 62584.1863265 62620.4477265 62641.9987265\n",
      " 62675.5225265 62727.0055265 62750.9511265 62759.3320265 62871.8762265\n",
      " 62883.5823265 62900.6109265 62934.1347265 62947.3048265 63003.5769265]\n",
      "rounded_abs_wOnsets_dta_ndarray= [62541 62562 62584 62620 62641 62675 62727 62750 62759 62871 62883 62900\n",
      " 62934 62947 63003]\n",
      "tape_num= 12\n",
      "wOnset_perTape_DF=          Word  Segment      onset\n",
      "1992     Soon       12   0.229476\n",
      "1993      her       12   0.504358\n",
      "1994      eye       12   0.658032\n",
      "1995     fell       12   0.974924\n",
      "1996       on       12   1.266236\n",
      "...       ...      ...        ...\n",
      "2124  happens       12  45.226353\n",
      "2125     when       12  45.677924\n",
      "2126      one       12  45.896829\n",
      "2127     eats       12  46.064448\n",
      "2128     cake       12  46.327849\n",
      "\n",
      "[137 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 22.9476225  50.4358225  65.8032225  97.4924225 126.6236225 144.1863225\n",
      " 151.5146225 180.1046225 222.0094225 268.7033225 279.4788225 292.6489225\n",
      " 329.3501225 356.1046225 364.4856225] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 68137\n",
      "abs_wOnsets_dta_ndarray= [68159.9476225 68187.4358225 68202.8032225 68234.4924225 68263.6236225\n",
      " 68281.1863225 68288.5146225 68317.1046225 68359.0094225 68405.7033225\n",
      " 68416.4788225 68429.6489225 68466.3501225 68493.1046225 68501.4856225]\n",
      "rounded_abs_wOnsets_dta_ndarray= [68159 68187 68202 68234 68263 68281 68288 68317 68359 68405 68416 68429\n",
      " 68466 68493 68501]\n",
      "subject_num= S13_Alice-natives_sfreq-100_raw.fif\n",
      "tape_num= 1\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "0        Alice        1   0.046000\n",
      "1          was        1   0.562721\n",
      "2    beginning        1   0.784543\n",
      "3           to        1   1.256929\n",
      "4          get        1   1.352925\n",
      "..         ...      ...        ...\n",
      "169         it        1  55.626219\n",
      "170        all        1  55.734441\n",
      "171     seemed        1  55.885951\n",
      "172      quite        1  56.195464\n",
      "173    natural        1  56.439728\n",
      "\n",
      "[174 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [  4.6     56.2721  78.4543 125.6929 135.2925 161.6327 231.0749 279.9918\n",
      " 293.8712 330.449  348.4082 365.4596 410.6667 427.4286 439.8799] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 853\n",
      "abs_wOnsets_dta_ndarray= [ 857.6     909.2721  931.4543  978.6929  988.2925 1014.6327 1084.0749\n",
      " 1132.9918 1146.8712 1183.449  1201.4082 1218.4596 1263.6667 1280.4286\n",
      " 1292.8799]\n",
      "rounded_abs_wOnsets_dta_ndarray= [ 857  909  931  978  988 1014 1084 1132 1146 1183 1201 1218 1263 1280\n",
      " 1292]\n",
      "tape_num= 2\n",
      "wOnset_perTape_DF=          Word  Segment      onset\n",
      "174       but        2   0.479840\n",
      "175      when        2   0.592424\n",
      "176       the        2   0.810806\n",
      "177    Rabbit        2   0.912952\n",
      "178  actually        2   1.415810\n",
      "..        ...      ...        ...\n",
      "346       was        2  58.681660\n",
      "347     going        2  58.861252\n",
      "348        to        2  59.148599\n",
      "349    happen        2  59.251158\n",
      "350      next        2  59.687374\n",
      "\n",
      "[177 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 47.9839572  59.2424402  81.0806402  91.2952402 141.5810402 191.8667402\n",
      " 238.5605402 255.7777402 321.1728402 343.1803402 359.5218402 379.0771402\n",
      " 438.4907402 563.7331402 574.7717402] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 6615\n",
      "abs_wOnsets_dta_ndarray= [6662.9839572 6674.2424402 6696.0806402 6706.2952402 6756.5810402\n",
      " 6806.8667402 6853.5605402 6870.7777402 6936.1728402 6958.1803402\n",
      " 6974.5218402 6994.0771402 7053.4907402 7178.7331402 7189.7717402]\n",
      "rounded_abs_wOnsets_dta_ndarray= [6662 6674 6696 6706 6756 6806 6853 6870 6936 6958 6974 6994 7053 7178\n",
      " 7189]\n",
      "tape_num= 3\n",
      "wOnset_perTape_DF=       Word  Segment      onset\n",
      "351  First        3   0.478723\n",
      "352    she        3   0.986270\n",
      "353  tried        3   1.201781\n",
      "354     to        3   1.501100\n",
      "355   look        3   1.604643\n",
      "..     ...      ...        ...\n",
      "530   this        3  61.269264\n",
      "531   time        3  61.591394\n",
      "532    she        3  62.071440\n",
      "533   said        3  62.263005\n",
      "534  aloud        3  62.538379\n",
      "\n",
      "[184 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 47.8723402  98.6269787 120.1780787 150.1099787 160.4642787 183.6337787\n",
      " 218.3548787 231.5249787 260.2596787 284.2052787 296.1780787 314.3827787\n",
      " 333.2936787 371.6065787 450.6269787] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 12706\n",
      "abs_wOnsets_dta_ndarray= [12753.8723402 12804.6269787 12826.1780787 12856.1099787 12866.4642787\n",
      " 12889.6337787 12924.3548787 12937.5249787 12966.2596787 12990.2052787\n",
      " 13002.1780787 13020.3827787 13039.2936787 13077.6065787 13156.6269787]\n",
      "rounded_abs_wOnsets_dta_ndarray= [12753 12804 12826 12856 12866 12889 12924 12937 12966 12990 13002 13020\n",
      " 13039 13077 13156]\n",
      "tape_num= 4\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "535       must        4   0.398386\n",
      "536         be        4   0.689428\n",
      "537    getting        4   0.840938\n",
      "538  somewhere        4   1.126350\n",
      "539       near        4   1.542892\n",
      "..         ...      ...        ...\n",
      "744      think        4  68.262788\n",
      "745        you        4  68.437370\n",
      "746      could        4  68.593016\n",
      "747     manage        4  68.760635\n",
      "748         it        4  69.129919\n",
      "\n",
      "[214 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 39.8385787  68.9427631  84.0937631 112.6349631 154.2891631 177.2879631\n",
      " 184.4716631 225.1791631 234.7573631 248.5197631 312.5805631 335.3288631\n",
      " 347.3015631 380.8254631 399.0834631] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 19037\n",
      "abs_wOnsets_dta_ndarray= [19076.8385787 19105.9427631 19121.0937631 19149.6349631 19191.2891631\n",
      " 19214.2879631 19221.4716631 19262.1791631 19271.7573631 19285.5197631\n",
      " 19349.5805631 19372.3288631 19384.3015631 19417.8254631 19436.0834631]\n",
      "rounded_abs_wOnsets_dta_ndarray= [19076 19105 19121 19149 19191 19214 19221 19262 19271 19285 19349 19372\n",
      " 19384 19417 19436]\n",
      "tape_num= 5\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "749        And        5   0.561146\n",
      "750       what        5   0.689847\n",
      "751         an        5   0.936113\n",
      "752   ignorant        5   1.115705\n",
      "753     little        5   1.618562\n",
      "..         ...      ...        ...\n",
      "937     saying        5  64.085883\n",
      "938         to        5  64.547542\n",
      "939        her        5  64.691215\n",
      "940       very        5  64.858834\n",
      "941  earnestly        5  65.313800\n",
      "\n",
      "[193 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 56.1145631  68.9847173  93.6113173 111.5705173 161.8562173 185.8018173\n",
      " 210.7547173 222.0976173 231.2984173 252.8494173 264.8222173 284.7481173\n",
      " 410.8902173 452.7950173 466.6011173] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 26042\n",
      "abs_wOnsets_dta_ndarray= [26098.1145631 26110.9847173 26135.6113173 26153.5705173 26203.8562173\n",
      " 26227.8018173 26252.7547173 26264.0976173 26273.2984173 26294.8494173\n",
      " 26306.8222173 26326.7481173 26452.8902173 26494.7950173 26508.6011173]\n",
      "rounded_abs_wOnsets_dta_ndarray= [26098 26110 26135 26153 26203 26227 26252 26264 26273 26294 26306 26326\n",
      " 26452 26494 26508]\n",
      "tape_num= 6\n",
      "wOnset_perTape_DF=        Word  Segment      onset\n",
      "942     Now        6   0.323245\n",
      "943   Dinah        6   0.568709\n",
      "944    tell        6   1.039809\n",
      "945      me        6   1.422938\n",
      "946     the        6   1.590557\n",
      "...     ...      ...        ...\n",
      "1134   ever        6  61.825659\n",
      "1135     to        6  62.101034\n",
      "1136    get        6  62.268653\n",
      "1137    out        6  62.436272\n",
      "1138  again        6  62.663755\n",
      "\n",
      "[197 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 32.3245298  56.8708743 103.9808743 142.2937743 159.0556743 173.0401743\n",
      " 238.0761743 260.0501743 284.7699743 320.6883743 337.9041743 353.4880743\n",
      " 473.9400743 496.7131743 597.2597743] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 32676\n",
      "abs_wOnsets_dta_ndarray= [32708.3245298 32732.8708743 32779.9808743 32818.2937743 32835.0556743\n",
      " 32849.0401743 32914.0761743 32936.0501743 32960.7699743 32996.6883743\n",
      " 33013.9041743 33029.4880743 33149.9400743 33172.7131743 33273.2597743]\n",
      "rounded_abs_wOnsets_dta_ndarray= [32708 32732 32779 32818 32835 32849 32914 32936 32960 32996 33013 33029\n",
      " 33149 33172 33273]\n",
      "tape_num= 7\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1139  Suddenly        7   0.586736\n",
      "1140       she        7   1.179532\n",
      "1141      came        7   1.418988\n",
      "1142      upon        7   1.694362\n",
      "1143         a        7   2.051148\n",
      "...        ...      ...        ...\n",
      "1312       her        7  61.175179\n",
      "1313      head        7  61.294906\n",
      "1314    though        7  61.675837\n",
      "1315       the        7  61.845655\n",
      "1316   doorway        7  61.965383\n",
      "\n",
      "[178 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 58.6736359 117.9532195 141.8988195 169.4362195 205.1148195 230.4974195\n",
      " 274.7968195 299.9396195 337.0553195 425.6539195 446.0076195 471.1505195\n",
      " 479.5315195 527.4226195 650.7423195] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 39059\n",
      "abs_wOnsets_dta_ndarray= [39117.6736359 39176.9532195 39200.8988195 39228.4362195 39264.1148195\n",
      " 39289.4974195 39333.7968195 39358.9396195 39396.0553195 39484.6539195\n",
      " 39505.0076195 39530.1505195 39538.5315195 39586.4226195 39709.7423195]\n",
      "rounded_abs_wOnsets_dta_ndarray= [39117 39176 39200 39228 39264 39289 39333 39358 39396 39484 39505 39530\n",
      " 39538 39586 39709]\n",
      "tape_num= 8\n",
      "wOnset_perTape_DF=        Word  Segment      onset\n",
      "1317    and        8   0.342000\n",
      "1318   even        8   0.453317\n",
      "1319     if        8   0.696699\n",
      "1320     my        8   0.960100\n",
      "1321   head        8   1.067855\n",
      "...     ...      ...        ...\n",
      "1487     do        8  55.663774\n",
      "1488   that        8  55.855338\n",
      "1489     in        8  56.190576\n",
      "1490      a        8  56.366154\n",
      "1491  hurry        8  56.444074\n",
      "\n",
      "[175 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 34.2        45.3316684  69.6698684  96.0099684 106.7854684 139.1120684\n",
      " 158.2684684 178.5899684 250.4589684 277.9963684 304.3431684 420.4725684\n",
      " 428.9338684 448.0099684 467.1664684] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 45355\n",
      "abs_wOnsets_dta_ndarray= [45389.2       45400.3316684 45424.6698684 45451.0099684 45461.7854684\n",
      " 45494.1120684 45513.2684684 45533.5899684 45605.4589684 45632.9963684\n",
      " 45659.3431684 45775.4725684 45783.9338684 45803.0099684 45822.1664684]\n",
      "rounded_abs_wOnsets_dta_ndarray= [45389 45400 45424 45451 45461 45494 45513 45533 45605 45632 45659 45775\n",
      " 45783 45803 45822]\n",
      "tape_num= 9\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1492        No        9   0.432017\n",
      "1493         I        9   1.046815\n",
      "1494        ll        9   1.198494\n",
      "1495      look        9   1.394026\n",
      "1496     first        9   1.753210\n",
      "...        ...      ...        ...\n",
      "1643      very        9  54.086271\n",
      "1644      soon        9  54.397564\n",
      "1645  finished        9  55.044094\n",
      "1646        it        9  55.423901\n",
      "1647       off        9  55.536451\n",
      "\n",
      "[156 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 43.2017422 104.6814784 119.8493784 139.4025784 175.3209784 213.3840784\n",
      " 235.1848784 270.9578784 278.9662784 311.8107784 344.1372784 353.1518784\n",
      " 362.0964784 398.0148784 447.0263784] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 51093\n",
      "abs_wOnsets_dta_ndarray= [51136.2017422 51197.6814784 51212.8493784 51232.4025784 51268.3209784\n",
      " 51306.3840784 51328.1848784 51363.9578784 51371.9662784 51404.8107784\n",
      " 51437.1372784 51446.1518784 51455.0964784 51491.0148784 51540.0263784]\n",
      "rounded_abs_wOnsets_dta_ndarray= [51136 51197 51212 51232 51268 51306 51328 51363 51371 51404 51437 51446\n",
      " 51455 51491 51540]\n",
      "tape_num= 10\n",
      "wOnset_perTape_DF=           Word  Segment      onset\n",
      "1648      What       10   1.051981\n",
      "1649         a       10   1.160767\n",
      "1650   curious       10   1.308375\n",
      "1651   feeling       10   2.146470\n",
      "1652      said       10   2.787119\n",
      "...        ...      ...        ...\n",
      "1830     could       10  59.244701\n",
      "1831       not       10  59.448239\n",
      "1832  possibly       10  59.855313\n",
      "1833     reach       10  60.430007\n",
      "1834        it       10  60.784961\n",
      "\n",
      "[187 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [105.1980798 116.0767    130.8375    214.647     278.7119    306.9088\n",
      " 367.0205    381.0893    411.0007    419.3817    460.2269    478.0483\n",
      " 490.7454    500.4853    645.6674   ] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 56821\n",
      "abs_wOnsets_dta_ndarray= [56926.1980798 56937.0767    56951.8375    57035.647     57099.7119\n",
      " 57127.9088    57188.0205    57202.0893    57232.0007    57240.3817\n",
      " 57281.2269    57299.0483    57311.7454    57321.4853    57466.6674   ]\n",
      "rounded_abs_wOnsets_dta_ndarray= [56926 56937 56951 57035 57099 57127 57188 57202 57232 57240 57281 57299\n",
      " 57311 57321 57466]\n",
      "tape_num= 11\n",
      "wOnset_perTape_DF=              Word  Segment      onset\n",
      "1835          she       11   0.274485\n",
      "1836        could       11   0.489783\n",
      "1837          see       11   0.701863\n",
      "1838           it       11   1.064477\n",
      "1839        quite       11   1.279987\n",
      "...           ...      ...        ...\n",
      "1987           to       11  53.797174\n",
      "1988         make       11  53.890245\n",
      "1989          one       11  54.211688\n",
      "1990  respectable       11  54.517325\n",
      "1991       person       11  55.331149\n",
      "\n",
      "[157 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 27.448538   48.9783265  70.1863265 106.4477265 127.9987265 161.5225265\n",
      " 213.0055265 236.9511265 245.3320265 357.8762265 369.5823265 386.6109265\n",
      " 420.1347265 433.3048265 489.5769265] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 62952\n",
      "abs_wOnsets_dta_ndarray= [62979.448538  63000.9783265 63022.1863265 63058.4477265 63079.9987265\n",
      " 63113.5225265 63165.0055265 63188.9511265 63197.3320265 63309.8762265\n",
      " 63321.5823265 63338.6109265 63372.1347265 63385.3048265 63441.5769265]\n",
      "rounded_abs_wOnsets_dta_ndarray= [62979 63000 63022 63058 63079 63113 63165 63188 63197 63309 63321 63338\n",
      " 63372 63385 63441]\n",
      "tape_num= 12\n",
      "wOnset_perTape_DF=          Word  Segment      onset\n",
      "1992     Soon       12   0.229476\n",
      "1993      her       12   0.504358\n",
      "1994      eye       12   0.658032\n",
      "1995     fell       12   0.974924\n",
      "1996       on       12   1.266236\n",
      "...       ...      ...        ...\n",
      "2124  happens       12  45.226353\n",
      "2125     when       12  45.677924\n",
      "2126      one       12  45.896829\n",
      "2127     eats       12  46.064448\n",
      "2128     cake       12  46.327849\n",
      "\n",
      "[137 rows x 3 columns]\n",
      "wOnset_datapoints_ndarray= [ 22.9476225  50.4358225  65.8032225  97.4924225 126.6236225 144.1863225\n",
      " 151.5146225 180.1046225 222.0094225 268.7033225 279.4788225 292.6489225\n",
      " 329.3501225 356.1046225 364.4856225] <class 'numpy.ndarray'>\n",
      "tape_start_datapoints_npINT64= 68575\n",
      "abs_wOnsets_dta_ndarray= [68597.9476225 68625.4358225 68640.8032225 68672.4924225 68701.6236225\n",
      " 68719.1863225 68726.5146225 68755.1046225 68797.0094225 68843.7033225\n",
      " 68854.4788225 68867.6489225 68904.3501225 68931.1046225 68939.4856225]\n",
      "rounded_abs_wOnsets_dta_ndarray= [68597 68625 68640 68672 68701 68719 68726 68755 68797 68843 68854 68867\n",
      " 68904 68931 68939]\n"
     ]
    }
   ],
   "source": [
    "## Trying the old way (Tape order 1st then subject cut 2nd)##\n",
    "\n",
    "# epochs parameters\n",
    "# Define your epoch window\n",
    "# We use a 1.5s window (including buffers for HHSA)\n",
    "tmin, tmax = -0.3, 1.2 \n",
    "\n",
    "for subject in SUBJECTS[0:3]:\n",
    "    print(\"subject_num=\", subject)\n",
    "    \n",
    "    # 1. Load Raw as an Eelbrain-compatible object\n",
    "    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)\n",
    "    raw_sfreq = raw.info['sfreq']\n",
    "    \n",
    "    # 2. Get the events for the 12 tapes\n",
    "    events_DICT = eelbrain.load.mne.events(raw)\n",
    "    \n",
    "    # Get the actual word onset based on EEG triggers datapoints\n",
    "    for i, stimulus_idx in enumerate(trial_indexes):\n",
    "        print(\"tape_num=\", i+1)\n",
    "        \n",
    "        # Find the word onset time based on the segment sequence    \n",
    "        wOnset_perTape_DF = word_onset_essentials_DF.loc[word_onset_essentials_DF[\"Segment\"] == stimulus_idx+1, :] #.to_numpy()\n",
    "        print(\"wOnset_perTape_DF=\", wOnset_perTape_DF)\n",
    "        #wOnset_time_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy() #*raw_sfreq\n",
    "        wOnset_datapoints_ndarray = wOnset_perTape_DF[\"onset\"].to_numpy()*raw_sfreq\n",
    "        print(\"wOnset_datapoints_ndarray=\", wOnset_datapoints_ndarray[0:15], type(wOnset_datapoints_ndarray))\n",
    "\n",
    "        # Get the tape start time\n",
    "        tape_start_datapoints_npINT64 = events_DICT[stimulus_idx]['i_start']\n",
    "        #tape_start_time_npFLOAT64 = tape_start_datapoints_npINT64 / raw_sfreq\n",
    "        print(\"tape_start_datapoints_npINT64=\", tape_start_datapoints_npINT64)\n",
    "        #print(\"tape_start_time_npFLOAT64=\", tape_start_time_npFLOAT64, type(tape_start_time_npFLOAT64))\n",
    "\n",
    "        # Get the actual word onset time by the triggers\n",
    "        #absolute_onsets_time_ndarray = tape_start_time_npFLOAT64 + wOnset_time_ndarray\n",
    "        #print(\"absolute_onsets_time_ndarray=\", absolute_onsets_time_ndarray[0:15])\n",
    "        abs_wOnsets_dta_ndarray = tape_start_datapoints_npINT64 + wOnset_datapoints_ndarray\n",
    "        print(\"abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "        \n",
    "        # To exclude the decimal but leave the integer along, and turn FLOAT into INT\n",
    "        abs_wOnsets_dta_ndarray = np.trunc(abs_wOnsets_dta_ndarray).astype(int)\n",
    "        print(\"rounded_abs_wOnsets_dta_ndarray=\", abs_wOnsets_dta_ndarray[0:15])\n",
    "        \"\"\"\n",
    "        # Make epochs\n",
    "        # Create the empty (N, 3) event matrix based on wOnset per tape\n",
    "        wOnset_events = len(abs_wOnsets_dta_ndarray)\n",
    "        wOnset_perTape_events = np.zeros((wOnset_events, 3), dtype=int)\n",
    "        # Fill the columns\n",
    "        wOnset_perTape_events[:, 0] = abs_wOnsets_dta_ndarray  # Column 0: The sample indices\n",
    "        wOnset_perTape_events[:, 2] = i+1           # Column 2: The event ID (e.g., 1)\n",
    "        print(wOnset_perTape_events)\n",
    "\n",
    "        \n",
    "        #word_perTape_epochs = mne.epochs(raw, tmin=tmin, tmax=tmax, baseline=None, events=wOnset_perTape_events)\n",
    "        tape_perTape_epochs = mne.Epochs(raw, events=wOnset_perTape_events, event_id=i+1, tmin=tmin, tmax=tmax, baseline=None, preload=True)\n",
    "        print(tape_perTape_epochs)\n",
    "    all_tapes_epochs.append(tape_word_epochs)\n",
    "\"\"\"        \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    for i, stimulus_idx in enumerate(trial_indexes):\n",
    "        # We find the start of the tape in the raw EEG\n",
    "        tape_start = events[i]['i_start']#['time']\n",
    "        print([i], tape_start, type(tape_start))\n",
    "        \n",
    "        # Get word onset times for this specific tape from your list\n",
    "        # word_onsets[stimulus_idx] is your impulse predictor\n",
    "        onsets = word_onsets[stimulus_idx].time.times[word_onsets[stimulus_idx].x > 0]\n",
    "        print(onsets, len(onsets))\n",
    "    \n",
    "        \n",
    "        # Convert relative word times to absolute EEG times\n",
    "        absolute_onsets = tape_start + onsets\n",
    "        \n",
    "        # Create segments (Epochs) directly in Eelbrain\n",
    "        # This is much faster and more accurate than manual padding/cropping\n",
    "        tape_word_epochs = mne.epochs(\n",
    "            raw, tmin=tmin, tmax=tmax, baseline=None, \n",
    "            events=absolute_onsets\n",
    "        )\n",
    "        all_tapes_epochs.append(tape_word_epochs)\n",
    "    \n",
    "    # 5. Combine all words from all tapes into one Dataset\n",
    "    # This 'ds' will have a column for 'EEG' and can have a column for 'Subject'\n",
    "    ds_all_words = eelbrain.combine(all_tapes_epochs)\n",
    "    \n",
    "    # Now you can access the data for HHSA:\n",
    "    # eeg_data = ds_all_words['eeg'].get_data()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a31e8146-2195-473f-82b8-cec8f4056a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stimuli\n",
    "# ------------\n",
    "# Make sure to name the stimuli so that the TRFs can later be distinguished\n",
    "# Load the gammatone-spectrograms; use the time axis of these as reference\n",
    "gammatone = [eelbrain.load.unpickle(PREDICTOR_audio_DIR / f'{stimulus}~gammatone-8.pickle') for stimulus in STIMULI]\n",
    "\n",
    "# Resample the spectrograms to 100 Hz (time-step = 0.01 s), which we will use for TRFs\n",
    "gammatone = [x.bin(0.01, dim='time', label='start') for x in gammatone]\n",
    "\n",
    "# Pad onset with 100 ms and offset with 1 second; make sure to give the predictor a unique name as that will make it easier to identify the TRF later\n",
    "gammatone = [eelbrain.pad(x, tstart=-0.100, tstop=x.time.tstop + 1, name='gammatone') for x in gammatone]\n",
    "\n",
    "# Load the broad-band envelope and process it in the same way\n",
    "envelope = [eelbrain.load.unpickle(PREDICTOR_audio_DIR / f'{stimulus}~gammatone-1.pickle') for stimulus in STIMULI]  # Load in the data\n",
    "envelope = [x.bin(0.01, dim='time', label='start') for x in envelope]\n",
    "envelope = [eelbrain.pad(x, tstart=-0.100, tstop=x.time.tstop + 1, name='envelope') for x in envelope]\n",
    "onset_envelope = [eelbrain.load.unpickle(PREDICTOR_audio_DIR / f'{stimulus}~gammatone-on-1.pickle') for stimulus in STIMULI]\n",
    "onset_envelope = [x.bin(0.01, dim='time', label='start') for x in onset_envelope]\n",
    "onset_envelope = [eelbrain.pad(x, tstart=-0.100, tstop=x.time.tstop + 1, name='onset') for x in onset_envelope]\n",
    "\n",
    "# Load onset spectrograms and make sure the time dimension is equal to the gammatone spectrograms\n",
    "gammatone_onsets = [eelbrain.load.unpickle(PREDICTOR_audio_DIR / f'{stimulus}~gammatone-on-8.pickle') for stimulus in STIMULI]\n",
    "gammatone_onsets = [x.bin(0.01, dim='time', label='start') for x in gammatone_onsets]\n",
    "gammatone_onsets = [eelbrain.set_time(x, gt.time, name='gammatone_on') for x, gt in zip(gammatone_onsets, gammatone)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6973146b-a3bb-45c9-b959-46dfd49466ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_onsets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m     tape_start \u001b[38;5;241m=\u001b[39m events[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_start\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;66;03m#['time']\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Get word onset times for this specific tape from your list\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# word_onsets[stimulus_idx] is your impulse predictor\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     onsets \u001b[38;5;241m=\u001b[39m word_onsets[stimulus_idx]\u001b[38;5;241m.\u001b[39mtime\u001b[38;5;241m.\u001b[39mtimes[word_onsets[stimulus_idx]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(onsets, \u001b[38;5;28mlen\u001b[39m(onsets))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    # Convert relative word times to absolute EEG times\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    absolute_onsets = tape_start + onsets\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m# eeg_data = ds_all_words['eeg'].get_data()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_onsets' is not defined"
     ]
    }
   ],
   "source": [
    "## Got to the wrong length of the word onset\n",
    "\n",
    "## Gemini-Produced\n",
    "## To segment the raw EEG based on word start time == word onset time ##\n",
    "# ... your loading code ...\n",
    "\n",
    "for subject in SUBJECTS:\n",
    "    # 1. Load Raw as an Eelbrain-compatible object\n",
    "    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)\n",
    "    raw_sfreq = raw.info['sfreq']\n",
    "    \n",
    "    # 2. Get the events for the 12 tapes\n",
    "    events = eelbrain.load.mne.events(raw)\n",
    "    trial_indexes = [STIMULI.index(stimulus) for stimulus in events['event']]\n",
    "    \n",
    "    # 3. Define your epoch window\n",
    "    # We use a 1.5s window (including buffers for HHSA)\n",
    "    tmin, tmax = -0.3, 1.2 \n",
    "    \n",
    "    # 4. Extract segments for each tape\n",
    "    # This creates a list of NDVars, each containing the epochs for one tape\n",
    "    all_tapes_epochs = []\n",
    "    \n",
    "    for i, stimulus_idx in enumerate(trial_indexes):\n",
    "        # We find the start of the tape in the raw EEG\n",
    "        tape_start = events[i]['i_start']#['time']\n",
    "        \n",
    "        # Get word onset times for this specific tape from your list\n",
    "        # word_onsets[stimulus_idx] is your impulse predictor\n",
    "        onsets = word_onsets[stimulus_idx].time.times[word_onsets[stimulus_idx].x > 0]\n",
    "        print(onsets, len(onsets))\n",
    "\n",
    "        \"\"\"\n",
    "        # Convert relative word times to absolute EEG times\n",
    "        absolute_onsets = tape_start + onsets\n",
    "        \n",
    "        # Create segments (Epochs) directly in Eelbrain\n",
    "        # This is much faster and more accurate than manual padding/cropping\n",
    "        tape_word_epochs = mne.epochs(\n",
    "            raw, tmin=tmin, tmax=tmax, baseline=None, \n",
    "            events=absolute_onsets\n",
    "        )\n",
    "        all_tapes_epochs.append(tape_word_epochs)\n",
    "\n",
    "    # 5. Combine all words from all tapes into one Dataset\n",
    "    # This 'ds' will have a column for 'EEG' and can have a column for 'Subject'\n",
    "    ds_all_words = eelbrain.combine(all_tapes_epochs)\n",
    "    \n",
    "    # Now you can access the data for HHSA:\n",
    "    # eeg_data = ds_all_words['eeg'].get_data()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905b472f-1749-48cb-8b3e-85b9c14126d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== WORD ONSET IS DOWN BELOW =====\n",
      "<NDVar 'word': 500 time>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Estimate TRFs\\n# -------------\\n# Loop through subjects to estimate TRFs\\nfor subject in SUBJECTS:\\n    subject_trf_dir = TRF_DIR / subject[:3]\\n    subject_trf_dir.mkdir(exist_ok=True)\\n    # Generate all TRF paths so we can check whether any new TRFs need to be estimated\\n    trf_paths = {model: subject_trf_dir / f'{subject[:3]} {model}.pickle' for model in models}\\n    # Skip this subject if all files already exist\\n    #if all(path.exists() for path in trf_paths.values()):\\n        #continue\\n    # Load the EEG data\\n    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)  # subject /\\n    # Band-pass filter the raw data between 0.5 and 20 Hz\\n    raw.filter(0.5, 20)\\n    # Interpolate bad channels\\n    raw.interpolate_bads()\\n    # Extract the events marking the stimulus presentation from the EEG file\\n    events = eelbrain.load.fiff.events(raw)\\n    # Not all subjects have all trials; determine which stimuli are present\\n    trial_indexes = [STIMULI.index(stimulus) for stimulus in events['event']]\\n    # Extract the EEG data segments corresponding to the stimuli\\n    trial_durations = [durations[i] for i in trial_indexes]\\n    eeg = eelbrain.load.fiff.variable_length_epochs(events, -0.100, trial_durations, connectivity='auto')  #, decim=5 #decim=5 meaning to resample to sfreq=100Hz\\n    # Since trials are of unequal length, we will concatenate them for the TRF estimation.\\n    eeg_concatenated = eelbrain.concatenate(eeg)\\n    \\n    pprint(models.items)\\n    \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the time point of every word onset time according to the Alice csv file ##\n",
    "\n",
    "# Load word tables and convert tables into continuous time-series with matching time dimension\n",
    "word_tables = [eelbrain.load.unpickle(PREDICTOR_word_DIR / f'{stimulus}~word.pickle') for stimulus in STIMULI]\n",
    "word_onsets = [eelbrain.event_impulse_predictor(gt.time, ds=ds, name='word') for gt, ds in zip(gammatone, word_tables)]\n",
    "\n",
    "#print(word_tables[-1]) #  the onset & offset of each tape\n",
    "print(\"===== WORD ONSET IS DOWN BELOW =====\")\n",
    "print(word_onsets[0][0:5])\n",
    "# This is the original Durations of 12 tapes based on gammatone\n",
    "# Extract the duration of the stimuli, so we can later match the EEG to the stimuli\n",
    "durations = [gt.time.tmax for stimulus, gt in zip(STIMULI, gammatone)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Estimate TRFs\n",
    "# -------------\n",
    "# Loop through subjects to estimate TRFs\n",
    "for subject in SUBJECTS:\n",
    "    subject_trf_dir = TRF_DIR / subject[:3]\n",
    "    subject_trf_dir.mkdir(exist_ok=True)\n",
    "    # Generate all TRF paths so we can check whether any new TRFs need to be estimated\n",
    "    trf_paths = {model: subject_trf_dir / f'{subject[:3]} {model}.pickle' for model in models}\n",
    "    # Skip this subject if all files already exist\n",
    "    #if all(path.exists() for path in trf_paths.values()):\n",
    "        #continue\n",
    "    # Load the EEG data\n",
    "    raw = mne.io.read_raw_fif(EEG_DIR / f'{subject}', preload=True)  # subject /\n",
    "    # Band-pass filter the raw data between 0.5 and 20 Hz\n",
    "    raw.filter(0.5, 20)\n",
    "    # Interpolate bad channels\n",
    "    raw.interpolate_bads()\n",
    "    # Extract the events marking the stimulus presentation from the EEG file\n",
    "    events = eelbrain.load.fiff.events(raw)\n",
    "    # Not all subjects have all trials; determine which stimuli are present\n",
    "    trial_indexes = [STIMULI.index(stimulus) for stimulus in events['event']]\n",
    "    # Extract the EEG data segments corresponding to the stimuli\n",
    "    trial_durations = [durations[i] for i in trial_indexes]\n",
    "    eeg = eelbrain.load.fiff.variable_length_epochs(events, -0.100, trial_durations, connectivity='auto')  #, decim=5 #decim=5 meaning to resample to sfreq=100Hz\n",
    "    # Since trials are of unequal length, we will concatenate them for the TRF estimation.\n",
    "    eeg_concatenated = eelbrain.concatenate(eeg)\n",
    "    \n",
    "    pprint(models.items)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f36ef5-03af-4e7e-84fa-68e3009bf109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import eelbrain\n",
    "from pathlib import Path\n",
    "import re\n",
    "import emd  # Ensure you ran: pip install EMD-signal\n",
    "from scipy.signal import hilbert\n",
    "\"\"\"\n",
    "\n",
    "## Conduct HHSA ##\n",
    "## Step one\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. CORE ANALYSIS FUNCTIONS\n",
    "# ==========================================\n",
    "def get_inst_freq_amp(signal, fs):\n",
    "    \"\"\"\n",
    "    Calculates instantaneous amplitude and frequency using Hilbert Transform.\n",
    "    \"\"\"\n",
    "    analytic_signal = hilbert(signal)\n",
    "    amplitude = np.abs(analytic_signal)\n",
    "    instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
    "    \n",
    "    # Derivative of phase for frequency\n",
    "    inst_freq = np.diff(instantaneous_phase) / (2.0 * np.pi) * fs\n",
    "    # Pad last point to match length\n",
    "    inst_freq = np.append(inst_freq, inst_freq[-1])\n",
    "    \n",
    "    return amplitude, inst_freq\n",
    "\n",
    "def run_hhsa(signal, fs):\n",
    "    \"\"\"\n",
    "    Performs Two-Layer EMD with Mirror Padding for short signals.\n",
    "    \"\"\"\n",
    "    if signal.ndim > 1: signal = signal.flatten()\n",
    "    \n",
    "    n_samples = len(signal)\n",
    "    \n",
    "    # Safety Check for empty/tiny signals\n",
    "    if n_samples < 10: return None\n",
    "\n",
    "    # --- A. MIRROR PADDING (Crucial for 114-point TRFs) ---\n",
    "    # We reflect the signal to make it 3x longer so EMD works\n",
    "    pad_width = n_samples\n",
    "    padded_signal = np.pad(signal, pad_width, mode='reflect')\n",
    "    \n",
    "    # --- B. LAYER 1: CARRIER DECOMPOSITION ---\n",
    "    try:\n",
    "        # Use emd.sift.sift (Quinn library)\n",
    "        # max_imfs=5 is enough for a simple TRF response\n",
    "        imfs_layer1 = emd.sift.sift(padded_signal, max_imfs=5)\n",
    "        imfs_layer1 = imfs_layer1.T  # Transpose to (N_IMFs, N_Time)\n",
    "    except Exception:\n",
    "        return None\n",
    "        \n",
    "    holo_points = []\n",
    "    \n",
    "    for imf_c in imfs_layer1:\n",
    "        # Un-pad: Extract the middle 'real' part\n",
    "        imf_c_real = imf_c[pad_width : pad_width + n_samples]\n",
    "        \n",
    "        # Get Carrier Frequency & Envelope\n",
    "        env_c, freq_c = get_inst_freq_amp(imf_c_real, fs)\n",
    "        \n",
    "        # Skip flat/empty IMFs\n",
    "        if np.sum(np.abs(env_c)) < 1e-10: continue\n",
    "\n",
    "        # --- C. LAYER 2: AM DECOMPOSITION ---\n",
    "        # Pad the envelope before sifting\n",
    "        padded_env = np.pad(env_c, pad_width, mode='reflect')\n",
    "        \n",
    "        try:\n",
    "            imfs_layer2 = emd.sift.sift(padded_env, max_imfs=5)\n",
    "            imfs_layer2 = imfs_layer2.T\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        for imf_am in imfs_layer2:\n",
    "            # Un-pad AM result\n",
    "            imf_am_real = imf_am[pad_width : pad_width + n_samples]\n",
    "            \n",
    "            _, freq_am = get_inst_freq_amp(imf_am_real, fs)\n",
    "            power_am = imf_am_real**2\n",
    "            \n",
    "            # Filter for valid graph range\n",
    "            mask = (freq_c > 0) & (freq_c < fs/2) & (freq_am > 0) & (freq_am < fs/2)\n",
    "            idx = np.where(mask)[0]\n",
    "            \n",
    "            if len(idx) > 0:\n",
    "                # Store [Carrier_Freq, AM_Freq, Power]\n",
    "                points = np.vstack((freq_c[idx], freq_am[idx], power_am[idx])).T\n",
    "                holo_points.append(points)\n",
    "                \n",
    "    if not holo_points: return None\n",
    "    return np.vstack(holo_points)\n",
    "\n",
    "# ==========================================\n",
    "# 2. EXPERIMENT CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_ROOT = Path(\"/Users/neuroling/Downloads/DINGHSIN_Results/Alice_Experiments_Results\")\n",
    "EEG_DIR = DATA_ROOT / 'EEG_ESLs' / 'Alice_ESL_ICAed_fif'\n",
    "TRF_DIR = DATA_ROOT / 'TRFs_ESLs'\n",
    "\n",
    "# Extract Subjects\n",
    "ESL_SUBJECTS = [path.name for path in EEG_DIR.iterdir() if re.match(r'n_2_S\\d*', path.name)]\n",
    "print(f\"Found {len(ESL_SUBJECTS)} subjects.\")\n",
    "\n",
    "# Settings\n",
    "TARGET_MODEL = 'Fzero'\n",
    "LIMIT_CARRIER = 5  # Hz\n",
    "LIMIT_AM = 5       # Hz\n",
    "NBINS = 50\n",
    "\n",
    "group_spectrum_sum = None\n",
    "n_subjects_processed = 0\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN LOOP\n",
    "# ==========================================\n",
    "print(f\"Starting Group HHSA on Model: {TARGET_MODEL}\")\n",
    "\n",
    "for subject in ESL_SUBJECTS:\n",
    "    subject_id_short = subject[4:8] # 'S010'\n",
    "    file_path = TRF_DIR / subject_id_short / f'{subject_id_short} {TARGET_MODEL}.pickle'\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # --- LOAD DATA ---\n",
    "        trf_obj = eelbrain.load.unpickle(file_path)\n",
    "        \n",
    "        # Handle h vs h_scaled\n",
    "        if hasattr(trf_obj, 'h_scaled'):\n",
    "            data_ndvar = trf_obj.h_scaled\n",
    "        else:\n",
    "            data_ndvar = trf_obj.h\n",
    "\n",
    "        # Handle Tuple (Partitioned Data)\n",
    "        if isinstance(data_ndvar, tuple):\n",
    "            data_ndvar = data_ndvar[0]\n",
    "\n",
    "        # --- EXTRACT PREDICTOR ---\n",
    "        trf_final = None\n",
    "        \n",
    "        # Strategy 1: Try name 'envelope'\n",
    "        try:\n",
    "            trf_final = data_ndvar['Fzero']\n",
    "        except:\n",
    "            # Strategy 2: Try Index\n",
    "            # If dims are [predictor, time], we grab the 2nd predictor (index 1)\n",
    "            # Assuming the order is [F0, Envelope]\n",
    "            dims = data_ndvar.dimnames\n",
    "            non_time_dims = [d for d in dims if d != 'time' and d != 'sensor']\n",
    "            \n",
    "            if len(non_time_dims) > 0:\n",
    "                dim_name = non_time_dims[0]\n",
    "                # Index 1 = Envelope. Change to 0 if you want F0.\n",
    "                trf_final = data_ndvar.sub(**{dim_name: 1}) \n",
    "            else:\n",
    "                trf_final = data_ndvar\n",
    "\n",
    "        # Average Sensors\n",
    "        if 'sensor' in trf_final.dimnames:\n",
    "            trf_final = trf_final.mean('sensor')\n",
    "            \n",
    "        trf_vector = trf_final.x\n",
    "        fs = 1.0 / trf_final.time.tstep\n",
    "        \n",
    "        # --- RUN HHSA ---\n",
    "        # print(f\"  Processing {subject_id_short}...\")\n",
    "        holo_data = run_hhsa(trf_vector, fs)\n",
    "        \n",
    "        if holo_data is None: \n",
    "            continue\n",
    "            \n",
    "        # --- DEBUG PRINT ---\n",
    "        # Print the average frequencies found for this subject\n",
    "        avg_fc = np.mean(holo_data[:, 0])\n",
    "        avg_fam = np.mean(holo_data[:, 1])\n",
    "        print(f\"  Subject {subject_id_short}: Avg Carrier={avg_fc:.2f}Hz, Avg AM={avg_fam:.2f}Hz\")\n",
    "        # -------------------\n",
    "        \n",
    "\n",
    "        # --- BIN RESULTS ---\n",
    "        fc = holo_data[:, 0]\n",
    "        fam = holo_data[:, 1]\n",
    "        power = holo_data[:, 2]\n",
    "        \n",
    "        H_subj, xedges, yedges = np.histogram2d(fc, fam, bins=NBINS, weights=power,\n",
    "                                           range=[[0, LIMIT_CARRIER], [0, LIMIT_AM]])\n",
    "        \n",
    "        if group_spectrum_sum is None:\n",
    "            group_spectrum_sum = H_subj\n",
    "        else:\n",
    "            group_spectrum_sum += H_subj\n",
    "            \n",
    "        n_subjects_processed += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Failed on {subject_id_short}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ==========================================\n",
    "# 4. PLOT FINAL RESULT\n",
    "# ==========================================\n",
    "if n_subjects_processed > 0:\n",
    "    group_avg_spectrum = group_spectrum_sum / n_subjects_processed\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Note: .T is used because imshow expects [rows, cols] = [y, x]\n",
    "    plt.imshow(group_avg_spectrum.T, origin='lower', cmap='jet', aspect='auto',\n",
    "               extent=[0, LIMIT_CARRIER, 0, LIMIT_AM], interpolation='gaussian')\n",
    "    \n",
    "    plt.colorbar(label='Modulation Power (a.u.)')\n",
    "    plt.xlabel('Carrier Frequency (Hz)')\n",
    "    plt.ylabel('AM Frequency (Hz)')\n",
    "    plt.title(f'Group HHSA (N={n_subjects_processed})\\nModel: {TARGET_MODEL} | Predictor: Envelope')\n",
    "    \n",
    "    # Diagonal line (1:1 coupling)\n",
    "    plt.plot([0, LIMIT_AM], [0, LIMIT_AM], 'w--', alpha=0.5)\n",
    "    plt.savefig(TRF_DIR / 'ESLs_Fzero_HHSA_TRF.png')\n",
    "    plt.show()\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"No subjects processed. Please check if pickle files contain the expected data structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ada7a-18b9-46c9-9480-435c3278c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import eelbrain\n",
    "from pathlib import Path\n",
    "import emd\n",
    "from scipy.signal import hilbert\n",
    "\"\"\"\n",
    "# Version 1 of IMFs in each layers (=second EMD)\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_ROOT = Path(\"/Users/neuroling/Downloads/DINGHSIN_Results/Alice_Experiments_Results\")\n",
    "TRF_DIR = DATA_ROOT / 'TRFs_ESLs'\n",
    "TARGET_MODEL = 'Fzero+envelope'\n",
    "TARGET_SUBJECT = 'S010'  # <--- Change this to look at different subjects\n",
    "\n",
    "# Which Layer 1 IMF do you want to decompose further?\n",
    "# 0 = First IMF (Fastest/Highest Freq), 1 = Second IMF, etc.\n",
    "TARGET_IMF_INDEX = 1 \n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD DATA (Robust Loading)\n",
    "# ==========================================\n",
    "file_path = TRF_DIR / TARGET_SUBJECT / f'{TARGET_SUBJECT} {TARGET_MODEL}.pickle'\n",
    "print(f\"Loading {TARGET_SUBJECT}...\")\n",
    "\n",
    "try:\n",
    "    trf_obj = eelbrain.load.unpickle(file_path)\n",
    "    \n",
    "    # Handle h vs h_scaled\n",
    "    if hasattr(trf_obj, 'h_scaled'):\n",
    "        data_ndvar = trf_obj.h_scaled\n",
    "    else:\n",
    "        data_ndvar = trf_obj.h\n",
    "        \n",
    "    # Handle Tuple\n",
    "    if isinstance(data_ndvar, tuple):\n",
    "        data_ndvar = data_ndvar[0]\n",
    "\n",
    "    # Extract Predictor (Strategy: Index)\n",
    "    # Assumes order is [F0, Envelope] -> Index 1 is Envelope\n",
    "    # If your model is just 'envelope', it might be Index 0.\n",
    "    try:\n",
    "        # Try finding 'envelope' by name\n",
    "        trf_final = data_ndvar['envelope']\n",
    "        pred_name = \"Envelope\"\n",
    "    except:\n",
    "        # Fallback to Index 1\n",
    "        dims = data_ndvar.dimnames\n",
    "        non_time_dims = [d for d in dims if d != 'time' and d != 'sensor']\n",
    "        if len(non_time_dims) > 0:\n",
    "            trf_final = data_ndvar.sub(**{non_time_dims[0]: 1})\n",
    "            pred_name = \"Predictor (Index 1)\"\n",
    "        else:\n",
    "            trf_final = data_ndvar\n",
    "            pred_name = \"Predictor\"\n",
    "\n",
    "    # Average Sensors\n",
    "    if 'sensor' in trf_final.dimnames:\n",
    "        trf_final = trf_final.mean('sensor')\n",
    "        \n",
    "    signal = trf_final.x\n",
    "    times = trf_final.time.times\n",
    "    fs = 1.0 / trf_final.time.tstep\n",
    "\n",
    "    if signal.ndim > 1: signal = signal.flatten()\n",
    "    print(f\"Data Loaded: {len(signal)} samples\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==========================================\n",
    "# 3. PROCESSING (With Mirror Padding)\n",
    "# ==========================================\n",
    "\n",
    "# --- A. Layer 1 Decomposition ---\n",
    "pad_width = len(signal)\n",
    "padded_signal = np.pad(signal, pad_width, mode='reflect')\n",
    "\n",
    "# Run EMD (Sift)\n",
    "imfs_layer1_padded = emd.sift.sift(padded_signal, max_imfs=5)\n",
    "\n",
    "# Un-pad Layer 1\n",
    "# Note: emd output is (Samples, IMFs)\n",
    "imfs_layer1 = imfs_layer1_padded[pad_width : pad_width + len(signal), :]\n",
    "n_imfs1 = imfs_layer1.shape[1]\n",
    "\n",
    "# --- B. Layer 2 Decomposition (Target IMF) ---\n",
    "# Extract the target IMF (e.g., IMF 0)\n",
    "target_imf_padded = imfs_layer1_padded[:, TARGET_IMF_INDEX]\n",
    "\n",
    "# Get Envelope (using Hilbert)\n",
    "analytic = hilbert(target_imf_padded)\n",
    "envelope_padded = np.abs(analytic)\n",
    "\n",
    "# Run EMD on Envelope\n",
    "imfs_layer2_padded = emd.sift.sift(envelope_padded, max_imfs=4)\n",
    "\n",
    "# Un-pad Layer 2\n",
    "envelope_real = envelope_padded[pad_width : pad_width + len(signal)]\n",
    "imfs_layer2 = imfs_layer2_padded[pad_width : pad_width + len(signal), :]\n",
    "n_imfs2 = imfs_layer2.shape[1]\n",
    "\n",
    "# ==========================================\n",
    "# 4. PLOTTING\n",
    "# ==========================================\n",
    "\n",
    "# --- FIGURE 1: LAYER 1 (Carrier) ---\n",
    "fig1, axes1 = plt.subplots(n_imfs1 + 1, 1, figsize=(10, 8), sharex=True)\n",
    "fig1.suptitle(f\"Layer 1: Carrier Decomposition ({TARGET_SUBJECT})\", fontsize=14)\n",
    "\n",
    "# Plot Original Signal\n",
    "axes1[0].plot(times, signal, 'k', label='Original TRF')\n",
    "axes1[0].set_title(f\"Original Signal: {pred_name}\")\n",
    "axes1[0].legend(loc='upper right')\n",
    "\n",
    "# Plot IMFs\n",
    "for i in range(n_imfs1):\n",
    "    ax = axes1[i + 1]\n",
    "    ax.plot(times, imfs_layer1[:, i], 'b')\n",
    "    ax.set_ylabel(f\"IMF {i+1}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes1[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(TRF_DIR / 'ESLs_S010_layer1-IMFs_HHSA_TRF.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- FIGURE 2: LAYER 2 (Amplitude Modulation) ---\n",
    "fig2, axes2 = plt.subplots(n_imfs2 + 1, 1, figsize=(10, 8), sharex=True)\n",
    "fig2.suptitle(f\"Layer 2: AM Decomposition of Carrier IMF {TARGET_IMF_INDEX+1}\", fontsize=14)\n",
    "\n",
    "# Plot Envelope\n",
    "axes2[0].plot(times, envelope_real, 'r', label=f'Envelope of IMF {TARGET_IMF_INDEX+1}')\n",
    "axes2[0].set_title(\"Amplitude Envelope (Input to Layer 2)\")\n",
    "axes2[0].legend(loc='upper right')\n",
    "\n",
    "# Plot AM IMFs\n",
    "for i in range(n_imfs2):\n",
    "    ax = axes2[i + 1]\n",
    "    ax.plot(times, imfs_layer2[:, i], 'g')\n",
    "    ax.set_ylabel(f\"AM IMF {i+1}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes2[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(TRF_DIR / 'ESLs_S010_layer2-IMF2_HHSA_TRF.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
